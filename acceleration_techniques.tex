\chapter{Strongly coupled methods for coupled fields}

In what follows the most common strongly coupled or implicit methods employd for the solution of coupled problems are presented.
This presentation seeks to provide a literature overview of the available approaches and uses as its basis the work by \cite{gatzhammer_efficient_2014}.

For the sake of clarity, the discretized equations of the thermo-mechanical problem under review are recovered here
\begin{gather}
    \mathbf M \ddot{\mathbf u}_{n+1} +\mathbf f_u^\text{\;int}(\bm \uptheta_{n+1}, \mathbf u_{n+1})-\mathbf f^\text{\;ext}_{u,n+1}=\mathbf 0, \label{eq:mech_problem}\\
    \mathbf C \dot{\bm \uptheta}_{n+1} +\mathbf f_\theta^\text{\;int}(\bm \uptheta_{n+1}, \mathbf u_{n+1})-\mathbf f^\text{\;ext}_{\theta,n+1}=\mathbf 0, \label{eq:therm_problem}
\end{gather}
where the complete definition of the material incremental discretized thermo-mechanical initial boundary value problem can be found in Chapter~\ref{chapter:thermo_mechanical_problem}.

As only partitioned approach are being considered, the thermal and mechanical problems are solved separetely.
To ease the discussion consider, consider the existence of two functions \(\pazocal U\) and \(\pazocal T\) that represent the solution of the mechanical and thermal problems with the temperature or the configuration fixed, respectively.
These so-called solvers satisfy
\begin{equation}
  \mathbf u = \pazocal U(\bm \uptheta),\quad
  \bm \uptheta = \pazocal T(\mathbf u).
\end{equation}
See Chapter~\ref{} for detailed information on them.

Two main ways of realizing a strongly coupled approach to the solution of the coupled problem are presented.
The first focuses on fixed-point solvers, and acceleration/stabilization techniques for them.
The second deals with approaches based on the Newton-Raphson method, with the main problem tackled being an efficient and accurate approximation to the Jacobian.


\section{Fixed-point approaches}

Consider that the $\bm \uptheta_n$ and $\mathbf u_n$ are both known and that $\bm \uptheta_{n+1}$ and $\mathbf u_{n+1}$ are now to be determined.
A superscript $k$ is introduced to denote the iterations concerning the loop of the implicit scheme. To prevent cluttering, the subscripts concerning the timestep are dropped.
There are two basic Schwarz procedures commonly employed in strongly coupled solution procedures. Both originate from domain decomposition.
They are the additive and the parallel Scharwz procedures.

\subsection{Block Jacobi or Schwarz additive}

If both the mechanical (Equation~\eqref{eq:mech_problem}) and the mechanical problem (Equation~\eqref{eq:therm_problem}) are solved in parallel the procedure is said to be Schwarz additive or block Jacobi, refering to the similarities with the procedure for the solution of linear systems of equations with the same name i.e.,
\begin{gather}
\mathbf u^{k+1} = \pazocal U(\bm \uptheta^k),\\
\bm \uptheta^{k+1} = \pazocal T(\mathbf u^k).
\end{gather}

Box~\ref{box:block_jacobi} shows the pseudo-code for the block Jacobi approach.

\begin{framedbox}[htb]
  \caption{Additive Schwarz procedure, also called block Jacobi, for one timestep.}
  \label{box:block_jacobi}
  \begin{center}
    \begin{minipage}{0.9\textwidth}
    \begin{enumerate}[(i)]
    \item \(\mathbf u^0 = \mathbf u_{n}\)
    \item \(\bm \uptheta^0 = \bm \uptheta_n\)
    \item \(k=0\)
    \item Enter the fixed-point loop
    \begin{enumerate}[(1)]
      \item Set fixed-point counter to zero: \(k=0\)
      \item Solve the mechanical problem at fixed temeperature \(\bm \uptheta^k\): \(\mathbf u^{k+1} = \pazocal U(\bm \uptheta^k)\)
      \item Solve the thermal problem at a fixed configuration \(\mathbf u^k\): \(\bm \uptheta^{k+1} = \pazocal T(\mathbf u^k)\)
      \item If the desired accuracy has not been reached, update \(k=k+1\) and go to step (1).

    \end{enumerate}
    \end{enumerate}
    \end{minipage}
  \end{center}
\end{framedbox}

\subsection{Block Gauss-Seidel or Schwarz multiplicative}

If the fields are solved sequentially, where the output of the first solver is used as the input for the second solver, the solution procedure is said to be Scharwz multiplicative or block Gauss-Seidel.
\begin{gather}
\mathbf u^{k+1}  = \pazocal U(\bm \uptheta^k),\\
\bm \uptheta^{k+1} = \pazocal T(\mathbf u^{k+1}).
\end{gather}
One of the fields must be chosen as the first and this may be important \citep{joosten_analysis_2009}.
Here, the focus is on the sequence coinciding with the isothermic splict, i.e., first the mechanical problem is solved at a fixed temeprature, and then the thermal problem is fixed at a fixed configuration.

Box~\ref{box:block_gauss_seidel} shows the pseudo-code for the block Gauss-Seidel approach.

\begin{framedbox}[htb]
  \caption{Multiplicative Schwarz procedure, also called block Gauss-Seidel, for one timestep.}
  \label{box:block_gauss_seidel}
  \begin{center}
    \begin{minipage}{0.9\textwidth}
    \begin{enumerate}[(i)]
    \item \(\mathbf u^0 = \mathbf u_{n}\)
    \item \(\bm \uptheta^0 = \bm \uptheta_n\)
    \item \(k=0\)
    \item Enter the fixed-point loop
    \begin{enumerate}[(1)]
      \item Set fixed-point counter to zero: \(k=0\)
      \item Solve the mechanical problem at fixed temeperature \(\bm \uptheta^k\): \(\mathbf u^{k+1} = \pazocal U(\bm \uptheta^k)\)
      \item Solve the thermal problem at a fixed configuration \(\mathbf u^{k+1}\): \(\bm \uptheta^{k+1} = \pazocal T(\mathbf u^{k+1})\)
      \item If the desired accuracy has not been reached, update \(k=k+1\) and go to step (1).

    \end{enumerate}
    \end{enumerate}
    \end{minipage}
  \end{center}
\end{framedbox}

As from a computational standpoint the block Jacobi and Gauss-Seidel approaches are very similar, the latter is used as the basis for the exposition that follows.
Their similarity comes from the fact that the block Jacobi method can be regard as the computation of two simultaneous decoupled block Gauss-Seidel iterations.

\paragraph{Convergence criteria}

For an iterative method to be useful, there must be reasonable criteria to determine its convergence.
With this in view, each full iteration of the strongly coupled scheme is written in a fixed-point notation as
\begin{equation} \label{eq:fixed_point_iteration}
\tilde{\bm \uptheta}^{k+1} = \pazocal T \left(\pazocal U(\bm \uptheta^k)\right),
\end{equation}
where $\tilde{\bullet}$ denotes a solution coming directly from the solver, whereas the same symbol without the tilde represents the quantity after post-processing.

The iteration residual is defined as
\begin{equation}
\mathbf r^{k+1} = \pazocal T\left(\pazocal U(\bm\uptheta^k)\right) - \bm\uptheta^k = \tilde{\bm \uptheta}^{k+1} - \bm\uptheta^k.
\end{equation}

It is equal to zero when the fixed-point $\bm \uptheta$ is reached, i.e.,
\begin{equation} \label{eq:residual_definition}
\mathbf r = \pazocal T\left(\pazocal U(\bm \uptheta)\right) - \bm \uptheta = \mathbf 0,
\end{equation}
and hence, a reasonable convergence measure for the fixed-point iteration.

The discrete  $L^{2}$-norm can be used to obtain a scalar representative of the vectorial residual \(\mathbf r^{k+1}=\left(r^{k+1,1}, \ldots, r^{k+1,m}\right)^{T}\) as
\begin{equation} \label{eq:absolute_residual_criterion}
\left\|\mathbf{r}^{k+1}\right\|_{L^{2}}=\sqrt{\sum_{i}\left(r^{k+1, i}\right)^{2}}=\sqrt{\sum_{i}\left(\tilde{\uptheta}^{k+1, i}-\uptheta^{k, i}\right)^{2}}.
\end{equation}

Directly using \eqref{eq:absolute_residual_criterion} yields an absolute convergence criterion
\begin{equation}
\left\|\boldsymbol{r} ^{k+1}\right\|_{L^{2}}<\epsilon_\mathrm{abs}.
\end{equation}
with $\epsilon_\mathrm{abs}>0$ as absolute convergence limit and convergence being obtained when the above condition renders valid.
However, since the absolute value of $r$ can change by orders of magnitude during one simulation, an absolute measure is not appropriate in all situations.
A relative measure solves this problem by setting the residual in relation with the current coupling iterate values as
\begin{equation}
\frac{\left\|\mathbf{r}^{k+1}\right\|_{L^{2}}}{\left\|\tilde{\bm{\uptheta}}^{k+1}\right\|_{L^{2}}}<\epsilon_\mathrm{rel}.
\end{equation}

Since a relative convergence measure can fail to work properly when the coupling iterate values are close to zero and rounding errors come into play, a combination of absolute and relative measure, where the absolute measure takes care of close to zero cases and the relative handles all other cases, is often a good choice.

\subsection{Acceleration techniques}

\section{Newton-Raphson schemes}

Newton-Raphson schemes, in the following denoted as Newton schemes, can be applied to the partitioned coupled fields either by considering the chained overall fixed-point notation (Equation~\eqref{eq:fixed_point_iteration}) or by writing up a blocked system consisting of coupled structural and fluid fixed-point iterations.

To apply a Newton scheme to the overall chained fixed-point iteration (Equation \eqref{eq:fixed_point_iteration}), the residual operator from (Equation~\eqref{eq:residual_definition}) is defined as
\begin{equation} \label{eq:residual_fixed_point}
\pazocal{R}\equiv\pazocal{U} \circ \pazocal {T}-\mathbf{I},
\end{equation}
where $\mathbf{I}$ is the identity matrix of suitable size.
The Newton scheme tries to find the root $\bm \uptheta$ such that $\pazocal R(\bm \uptheta)=0$, assuming a linear behavior of $\pazocal R$ described by its Jacobi matrix $J_{\pazocal R}(\bm \uptheta)$.

An iterative scheme, consisting of repeatedly solving
\begin{equation} \label{eq:newton_raphson}
J_{\pazocal R}\left(\bm \uptheta^{k}\right) \Delta \bm \uptheta^{k}=-\pazocal R\left(\bm \uptheta^{k}\right)
\end{equation}
for $\Delta \bm \uptheta^{k}$ and updating the iterate
\begin{equation}
\bm \uptheta^{k+1}= \bm \uptheta^{k}+\Delta \bm \uptheta^{k}
\end{equation}
is performed.
In case of writing up a separate fixed-point iteration for the different fields, a thermal residual operator $\pazocal{R}_{u}(\mathbf{u}, \bm{\uptheta})$ and, in addition, a mechanical residual operator $\pazocal{R}_{\theta}(\mathbf{u}, \bm{\uptheta})$ are obtained
\begin{gather}
\pazocal{R}_{u}(\mathbf{u}, \bm{\uptheta})=\mathbf{u}-\pazocal{U}(\bm{\uptheta})=0, \\
\pazocal{R}_{\theta}(\mathbf{u}, \bm{\uptheta})=\bm{\uptheta}-\pazocal{T}(\mathbf{u})=0.
\end{gather}

From this, a block Newton iteration can be written as
\begin{equation} \label{eq:block_newton_raphson}
\left[\begin{array}{ll}J_{\pazocal R_{u}}\left(\mathbf{u}^{k}\right) & J_{\pazocal{R}_{u}}\left(\boldsymbol{\theta}^{k}\right) \\[7pt] J_{\pazocal{R}_{\theta}}\left(\mathbf{u}^{k}\right) & J_{\pazocal{R}_{\theta}}\left(\boldsymbol{\theta}^{k}\right)\end{array}\right]
\left\{\begin{array}{c}\Delta \mathbf{u}^{k} \\ \Delta \bm{\uptheta}^{k}\end{array}\right\}
=-\left\{\begin{array}{l}\pazocal{R}_{u}\left(\mathbf{u}^{k}, \boldsymbol{\theta}^{k}\right) \\ \pazocal{R}_{\theta}\left(\mathbf{u}^{k}, \boldsymbol{\theta}^{k}\right)\end{array}\right\},
\end{equation}
and the update of the iteration variables reads
\begin{equation}
\left\{\begin{array}{l}
\mathbf{u}^{k+1} \\
\boldsymbol{\theta}^{k+1}
\end{array}\right\}=\left\{\begin{array}{l}
\mathbf{u}^{k} \\
\boldsymbol{\theta}^{k}
\end{array}\right\}+\left\{\begin{array}{c}
\Delta \mathbf{u}^{k} \\
\Delta \boldsymbol{\theta}^{k}
\end{array}\right\}.
\end{equation}

Every iteration of the Newton scheme involves at least one invocation of the thermal and mechanical solvers when computing $\pazocal{R}\left(\mathbf{u}^{k}\right)$ or both $\pazocal{R}_{u}\left(\mathbf{u}^{k}, \boldsymbol{\theta}^{k}\right)$ and $\pazocal{R}_{\theta}\left(\mathbf{u}^{k}, \boldsymbol{\theta}^{k}\right)$.

The critical point for black box equation coupling is how to obtain the derivative information in the Jacobi matrices in \eqref{eq:newton_raphson} or \eqref{eq:block_newton_raphson}.
In the following, methods are presented which find approximations for the required Jacobian times vector products in different ways.

\subsection{GMRES-inspired approach}

In \cite{michler_interface_2005}, a Krylov-subspace method is proposed in the context of FSI.
In the present document, this mehod is adapted to the thermo-mechanical problem.
It approximates the solution $\Delta \bm{\uptheta}^{k}$ of system \eqref{eq:newton_raphson}, which results in a Newton-Krylov solver for the partitioned thermo-mechanical system.
This particular nomenclature is however debatable.
\cite{kuttler_vector_2009} argues that the correct term for this approach should be instead a “Krylov-based vector extrapolation” method. \jvc{Later take a closer look a this and link to the section on vector extrapolation.}

% A review of Newton-Krylov methods in general is given in [93].

For the GMRES-inspired approach, the Krylov-based subspace of order $m$ is written as
\begin{equation}
\pazocal{K}_{m}:=\operatorname{span}\left\{\boldsymbol{\theta}^{*}_i-\boldsymbol{\theta}^{k}\right\}_{i=1}^{m}=\operatorname{span}\left\{\Delta \boldsymbol{\theta}^*_{i}\right\}_{i=1}^{m},
\end{equation}
where $\Delta \bm{\uptheta}^*_{i}:=\bm{\uptheta}^*_{i}-\bm{\uptheta}^{k}$ and the $\bm{\uptheta}^*_{i}$ are generated during the Krylov iterations as $\bm{\uptheta}^*_{i+1}=\pazocal{T} \circ \pazocal U\left(\bm{\uptheta}^*_{i}\right)$ . $\bm{\uptheta}^{k}$ is associated to the outer Newton iteration and fixed during the Krylov iterations.

The residual for the $m^\mathrm{th}$ Krylov iteration can be written as
\begin{equation}
J_\pazocal{R}(\boldsymbol \theta^k)\Delta\boldsymbol \theta^*_m - \pazocal R(\boldsymbol \theta^k).
\end{equation}
It is desirable that the $L_2$-norm of this residual be as small as possible.
Approximating $\Delta \bm \uptheta^*_i$ using $\pazocal K_m$, as $\sum_{i=1}^m \alpha_i \Delta \bm \uptheta^*_i$, the coefficients $\alpha_i$ are thus found as
\begin{equation} \label{eq:gmres_1st_ls}
\bar{\boldsymbol \alpha} = \arg\min_{\boldsymbol\alpha} \left\|J_\pazocal{R}(\bm \uptheta^k)\sum_{i=1}^m \alpha_i \Delta\bm \uptheta^i - \pazocal R(\bm \uptheta^k)\right\|.
\end{equation}

Since the Jacobian matrix $J_{\pazocal R}$ is assumed not to be accessible, a finite-difference approach
\begin{equation}
J_{\pazocal{R}}\left(\boldsymbol{\theta}^{k}\right) \Delta \boldsymbol{\theta}^*_{i} \approx \pazocal{R}\left(\boldsymbol{\theta}^*_{i}\right)-\pazocal{R}\left(\boldsymbol{\theta}^{k}\right)
\end{equation}
is employed.
Using the notation
\begin{equation}
\mathbf r^*_i = \pazocal R(\boldsymbol \theta^*_i),\quad
\mathbf r^k \equiv  \pazocal R(\boldsymbol \theta^k),\quad
\Delta \mathbf r_i^* \equiv \mathbf r_i^* - \mathbf r^k,
\end{equation}
Equation~\eqref{eq:gmres_1st_ls} can be rewritten in a more compact form as
\begin{equation} \label{eq:gmres_ls_condition}
\bar{\boldsymbol \alpha} = \arg\min_{\boldsymbol \alpha} \left\|\mathbf r^k+\sum_{i=1}^m\alpha_i\Delta\mathbf r^*_i\right\|,
\end{equation}
with the $\alpha_i$ determined in a least-squares sense.
The quality of the approximation is determined from the norm of the residual
\begin{equation} \label{eq:gmres_residual}
\xi=\left\|\boldsymbol{r}^{k}+\sum_{i=1}^{m} \bar{\alpha}_{i} \Delta \boldsymbol{r}^*_{i}\right\|.
\end{equation}

Orthogonalizing a new Krylov-vector $\Delta \bm{\uptheta}^*_{m}$ with respect to all previous ones using the Arnoldi process, as shown in Box~\ref{box:arnoldi_process}, completes the connection to the generalized minimal residual method (GMRES).
Furthermore, to stabilize the solver subiterations when setting up the Krylov-subspace approximation, constant underrelaxation by $\omega$ can be employed as
\begin{equation}
\Delta \boldsymbol{\theta}^*_{m}=\omega \Delta \boldsymbol{\theta}^*_{m}.
\end{equation}

\begin{framedbox}[htbp]
  \caption{Arnoldi process to orthonormalize temperature deltas}
  \label{box:arnoldi_process}
  \begin{center}
    \begin{minipage}{0.9\textwidth}
    \begin{enumerate}[(i)]
      \item \(j=1\)
      \item \(\Delta \bm\uptheta^*_m = \Delta \bm\uptheta^*_m - \Delta \bm\uptheta^*_j\frac{\Delta\bm\uptheta^*_m\cdot\Delta\bm\uptheta^*_j}{\Delta\bm \uptheta^*_j\cdot\Delta\bm\uptheta^*_j}\)
      \item \(j=j+1\)
      \item If \(j<m-1\) go to Step (ii)
      \item \(\Delta\bm \uptheta^*_m = \Delta\bm\uptheta^*_m/\|\Delta \bm \uptheta^*_m\|\)
    \end{enumerate}
    \end{minipage}
  \end{center}
\end{framedbox}


Box~\ref{box:gmres_inspired} puts all parts of the GMRES-inpired method for one coupling timestep in proper order. The Krylov-vectors can be reused between different coupling iterations $k$ or even between different timesteps $n$.

\begin{framedbox}[htbp]
  \caption{Timestep \(n\) of the GMRES-inspired approach.}
  \label{box:gmres_inspired}
  \begin{center}
    \begin{minipage}{0.9\textwidth}
    \begin{enumerate}[(i)]
    \item \(k=0\)
    \item If no previous timesteps are to be reused or \(n=0\) then
    \begin{itemize}
      \item Clear all \(\Delta \bm \uptheta^*_i\), \(\Delta \mathbf r^*_i\)
      \item \(m=0\)
    \end{itemize}
    \item \(\bm \uptheta^*_1 = \pazocal T\circ \pazocal U(\bm \uptheta^0)\)
    \item \(\mathbf r^0 = \bm \uptheta^*_1 - \bm \uptheta^0\)
    \item Enter the Newton loop
    \begin{enumerate}[(1)]
      \item If no previous iterations are to be reused
      \begin{itemize}
        \item Clear all \(\Delta \bm\uptheta^*_i\), \(\Delta \mathbf r^*_i\)
        \item \(m=0\)
        \item \(\xi = \|\mathbf r^*_k\|\)
      \end{itemize}
      else
      \begin{itemize}
        \item Compute \(\bar{\mathbf\alpha}\) (Equation~\eqref{eq:gmres_ls_condition}) and \(\xi\) (Equation~\eqref{eq:gmres_residual})
        \item \(\bm \uptheta^*_{m+1} = \bm \uptheta^*_1\)
      \end{itemize}
      \item Enter the Krylov loop
      \begin{enumerate}[(a)]
        \item \(m=m+1\)
        \item \(\Delta \bm\uptheta^*_m = \bm\uptheta^*_m - \bm\uptheta^k\)
        \item Orthogonalize (Box~\ref{box:arnoldi_process}) and relax \(\Delta \bm\uptheta^*_m\) (Equation~\eqref{})
        \item \(\bm\uptheta^*_m = \bm\uptheta^k + \Delta\bm\uptheta^*_m\)
        \item \(\bm \uptheta^*_{m+1} = \pazocal T \circ \pazocal U(\bm \uptheta^*_m)\)
        \item \(\Delta \mathbf r^*_m = (\bm\uptheta^*_{m+1} - \bm \uptheta^*_m) - \mathbf r^k\)
        \item Compute \(\bar{\mathbf\alpha}\) (Equation~\eqref{eq:gmres_ls_condition}) and \(\xi\) (Equation~\eqref{eq:gmres_residual})
        \item If convergence has not been reached, \(\xi>\epsilon_2\), go to Step (a)
      \end{enumerate}
    \item \(\bm\uptheta^{k+1} = \bm\uptheta^k + \sum_{i=1}^m \bar{\alpha}_i \Delta\bm\uptheta_i\)
    \item \(k=k+1\)
    \item \(\bm\uptheta^*_1 = \pazocal T\circ \pazocal U(\bm\uptheta^{k})\)
    \item \(\mathbf r^k = \bm\uptheta^*_1 - \bm \uptheta^k\)
    \item If convergence has not been reached, \(\|\mathbf r^k\| > \epsilon_1\), go to Step (1)
    \end{enumerate}
    \end{enumerate}
    \end{minipage}
  \end{center}
\end{framedbox}

% \Floatbarrier

\subsection{Quasi-Newton Method}

An interface quasi-Newton method based on Equation~\eqref{eq:residual_fixed_point} and Equation~\eqref{eq:newton_raphson} is presented in \cite{degroote_joris_development_2010} for FSI.
The method is called interface quasi Newton with approximation of the inverse of the interface Jacobian matrix by least squares (IQN-ILS).
Its origin, the IBQN-LS method presented in \cite{vierendeels_implicit_2007}, is derived from the block Newton method Equation~\eqref{eq:block_newton_raphson} and uses two reduced order models for fluid and structure to approximate their interface Jacobians directly.
In contrast, the IQN-ILS method provides one reduced order model for the inverse of the overall interface Jacobian matrix of the Newton system (Equation~\eqref{eq:newton_raphson}) applied to the right-hand side vector.
In the following, the method is adapted to the thermo-mechanical problem under analysis and are based on the description of the method in \cite{gatzhammer_efficient_2014}.
It is henceforth denominated as quasi Newton with approximation of the inverse of the Jacobian matrix by least squares (QN-LS).

As already stated, the Jacobian in Equation~\eqref{eq:newton_raphson} is approximated using a reduced order model.
Since computing \(\pazocal R\) is expensive the estimate for the inverse of the Jacobian tries to take the most advantage from the values already computed so far during the iterative procedure.
Appropriate input and output deltas are used to construct it.
In the residual formulation (Equation~\eqref{eq:residual_fixed_point}) of the chained solvers, the input is given by the temperature values $\bm \uptheta$ and the output by the residual $\mathbf{r}$.
Input and output deltas are defined by
\begin{gather}
\Delta \mathbf{r}^{k}=\mathbf{r}^{k+1}-\mathbf{r}^{k} \\
\Delta \tilde{\bm{\uptheta}}^{k}=\tilde{\bm{\uptheta}}^{k+1}-\tilde{\bm{\uptheta}}^{k}
\end{gather}
i.e., not the actual input deltas $\Delta \bm{\uptheta}$ are gathered, but the intermediate chained solver outputs $\tilde{\bm{\uptheta}}$ are used to construct the $\Delta \tilde{\bm{\uptheta}}$.
This enables the approximation the inverse of the Jacobian applied to the right-hand side vector of the Newton system, as is shown below.

For convenience consider the matrices
\begin{align}
\mathbf{V}_{k} &=\left(\Delta \mathbf{r}^{k}, \Delta \mathbf{r}^{k-1}, \ldots, \Delta \mathbf{r}^{1}, \mathbf{V}^{n-1}, \ldots, \mathbf{V}^{n-l}\right)\label{eq:matrix_delta_res} \\
\mathbf{W}{k} &=\left(\Delta \tilde{\bm{\uptheta}}^{k}, \Delta \tilde{\bm{\uptheta}}^{k-1}, \ldots, \Delta \tilde{\bm{\uptheta}}^{1}, \mathbf{W}^{n-1}, \ldots, \mathbf{W}^{n-l}\right) \label{eq:matrix_delta_theta}
\end{align}
with inversed ordering of the columns compared to the original IBQN-LS method (see Section~\ref{}).
The matrices \(\mathbf{V}^{n-1}, \ldots, \mathbf{V}^{n-l}\) and \(\mathbf{W}^{n-1}, \ldots, \mathbf{W}^{n-l}\) represent the (optional) reuse of columns from \(l\) old timesteps.
Reusing old timestep information can improve the approximation of the inverse of the Jacobian, particularly in the first iterations of a timestep.

% The quantity to be approximated is
% \begin{equation}
%   \hat{J}_{\pazocal R}^{-1}(\bm\uptheta_k)(-\pazocal R(\bm\uptheta_k)),
% \end{equation}
% i.e., the solution to the system of equations in Equation~\eqref{eq:newton_raphson}.
% Employing a slightly different notation, it can be written as
% \begin{equation}
%   \hat{J}_{\pazocal R}^{-1}(\bm\uptheta_k)\Delta\mathbf r^k,
% \end{equation}
% assuming \(\mathbf r^{k+1}=\mathbf 0\) and thus \(\Delta \mathbf r^k = \mathbf r^{k+1} - \mathbf r^k = \mathbf 0 -\mathbf r^k\).
%
% For every pair \(\Delta \mathbf r^i\) and \(\Delta \bm \uptheta^i\), there is a corresponding estimate for the inverse of the Jacobian at \(\bm \uptheta^i\), which can be found from
% \begin{equation}
%   \hat{J}^{-1}_\pazocal{R}(\bm\uptheta^i) \Delta \mathbf r^i = \Delta \bm\uptheta^i.
% \end{equation}
%
% Thus, the inverse of the Jacobian at \(\bm \uptheta^k\) is to be approximated by a linear combination of the available estimates for the Jacobian.
% To avoid solving the linear system, the vector \(\hat{J}^{-1}_\pazocal{R}(\uptheta^k)\Delta \mathbf r^k\) is considered instead, yielding
% \begin{equation}
%   \hat{J}^{-1}_\pazocal{R}(\bm\uptheta^k) \Delta \mathbf r^k = \sum_{i=1}^k c_i \hat{J}^{-1}_\pazocal{R}(\uptheta^i) \Delta \mathbf r^i.
% \end{equation}
% From Equation~\eqref{} we can substitute and find
% \begin{equation}
%   \sum_{i=1}^k c_i \hat{J}^{-1}_\pazocal{R}(\uptheta^i)\Delta \mathbf r^i = \sum_{i=1}  c_i \Delta \bm\uptheta^i = \mathbf W_K\mathbf c,
% \end{equation}
% thus
% \begin{equation}
%   \hat{J}^{-1}_\pazocal{R}(\bm\uptheta^k) \Delta \mathbf r^k = \mathbf W_k \mathbf c.
% \end{equation}
%
% An appropriate choice for \(\mathbf c\) will be one that leads to a good approximation of \(J^{-1}_\pazocal{R}(\bm \uptheta^k)\).
% Given that each approximation to the inverse of the Jacobian is related to a \(\Delta \mathbf r\)), and assuming that it is a good aroximation whatever the \(\Delta \mathbf r\) \(\Delta \mathbf r^k\) by the available \(\Delta \mathbf r^i\), e.g., the minimization of


A linear combination of residual deltas from \(V_{k}\) is used to approximate
\begin{equation}
\Delta \mathbf r \approx \mathbf V_{k} \mathbf c
\end{equation}
which is defined to be \(\Delta \mathbf{r}=\mathbf{0}-\mathbf{r}^{k+1}\), to get a new residual \(\mathbf{r}^{k+2}\) that is zero.
The desired linear combination is found by the least-squares solution
\begin{equation}
\mathbf{c}=\underset{\bar{\mathbf c}}{\arg \min }\left\|\Delta \mathbf{r}-\mathbf{V}_{k} \tilde{\mathbf{c}}\right\|
\end{equation}
which can be computed, e.g., by an economy size \(Q R\)-decomposition.
Here, a possible problem is that the columns of \(\mathbf{V}_{k}\) can become linearly dependent such that a column removal becomes necessary.
Adding new columns as first columns, as in Equations~\eqref{eq:matrix_delta_res} and \eqref{eq:matrix_delta_theta}, enables to detect and remove older columns as linearly dependent during the least-squares solution.
This is particularly important when columns are reused from old timesteps since, then, the newer columns better represent the current state of the residual operator (Equation~\eqref{eq:residual_fixed_point}).

A \(\Delta \tilde{\bm{\uptheta}}\) corresponding to a \(\Delta \mathbf{r}\) is then constructed by using the coefficients \(\mathbf{c}\) in combination with the matrix \(\mathbf{W}_{k}\) as
\begin{equation} \label{eq:linear_approx_output}
\Delta \tilde{\bm{\uptheta}}=\mathbf{W}_{k} \mathbf{c}.
\end{equation}
To construct an approximation for the inverse of the interface Jacobian, the residual formulation (Equation~\eqref{eq:residual_definition}) using \(\Delta \mathbf{r}\), \(\Delta \tilde{\bm{\uptheta}}\), and \(\Delta \bm{\uptheta}\), reading
\begin{equation} \label{eq:relation_res_out}
  \Delta \mathbf r = \Delta \tilde{\bm\uptheta} - \Delta\bm\uptheta.
\end{equation}
Inserting Equation~\eqref{eq:linear_approx_output} into Equation~\eqref{eq:relation_res_out} yields the desired approximation for the inverse of the residue Jacobian \(\hat{J}^{-1}_\pazocal{R}(\bm\uptheta)\) applied to the right-hand side residual delta
\begin{equation}
  \Delta \bm\uptheta^{k+1} = - \hat{J}^{-1}_\pazocal{R}(\bm\uptheta^k)\mathbf r^{k+1} \equiv \mathbf W_k \mathbf c + \mathbf r^{k+1}.
\end{equation}
Box~\eqref{box:quasi_newton} shows the QN-LS method for one timestep of the couple simulation.
When no delta columns are available yet, constant relaxation is used once to ensure stability.


\subsection{Block Quasi-Newton Method}

Linear reduced order models for the fluid solver and the structure solver are used in [172] to speed up the convergence of the FSI coupling iterations. As important feature, the reduced order models are set up from solver input and output deltas (or sensitivities) during the coupling iterations. The resulting method for two black box solvers is called interface block quasi-Newton method with least-squares approximation (IBQN-LS) in [42].

It application to a thermo-mechanical solver is detailed in what follows based on the version of the method for FSI presented in \cite{gatzhammer}.

In this method, the block Newton system (Equation~\eqref{}) is approximated by

$$
\left[\begin{array}{cc}
\widehat{\boldsymbol{U}^{\prime}} & -\boldsymbol{I} \\
-\boldsymbol{I} & \widehat{\boldsymbol{T}^{\prime}}
\end{array}\right]\left\{\begin{array}{c}
\Delta \hat{\mathbf{u}}_k \\
\Delta \hat{\boldsymbol{\theta}}_k
\end{array}\right\}=\left\{\begin{array}{l}
\pazocal{R}_{u}(\mathbf{u}_k, \boldsymbol{\theta}_k) \\
\pazocal{R}_{\theta}(\mathbf{u}_k, \boldsymbol{\theta}_k)
\end{array}\right\}
$$

where $\widehat{\boldsymbol{U}^{\prime}}$ and $\widehat{\boldsymbol{T}}^{\prime}$ are linear reduced order models for the Jacobians of the mechanical and thermal solvers.

Solving Equation~\eqref{}, $\Delta\hat{\mathbf u}$ and $\Delta \hat{\boldsymbol \theta}$  are found to be

$$
\left(\boldsymbol{I}-\widehat{\boldsymbol{T}}^{\prime} \widehat{\boldsymbol{U}}^{\prime}\right) \Delta \hat{\mathbf{u}}_k=-\pazocal{R}_{\theta}\left(\mathbf{u}_k, \boldsymbol{\theta}_k\right)-\widehat{\boldsymbol{T}}^{\prime} \pazocal{R}_{u}\left(\mathbf{u}_k, \boldsymbol{\theta}_k\right)\\
\left(\boldsymbol{I}-\widehat{\boldsymbol{U}}^{\prime} \widehat{\boldsymbol{T}}^{\prime}\right) \Delta \hat{\boldsymbol{\theta}}_k=-\pazocal{R}_{u}\left(\mathbf{u}_k, \boldsymbol{\theta}_k\right)-\widehat{\boldsymbol{U}}^{\prime} \pazocal{R}_{\theta}\left(\mathbf{u}_k, \boldsymbol{\theta}_k\right)
$$

Since neither $\hat{\mathbf U}’$ nor $\hat{\mathbf T}'$ are known they must be approximated. A Gauss-Seidel like scheme will be employed, in the sense that the equations are solved one after the other with the most up to date values for $\mathbf u$ and $\boldsymbol \theta$ being used for the approximations. Thus assuming that $\mathbf u_k$ and $\boldsymbol \theta_k$ are known, they are used to compute the approximations $\hat{\mathbf U}_k$ and $\hat{\mathbf T}_k$ and $\mathbf u_{k+1}$ is found from

$$
\left(\boldsymbol{I}-\widehat{\boldsymbol{T}}_{k}^{\prime} \widehat{\boldsymbol{U}}_{k}^{\prime}\right) \Delta \hat{\mathbf{u}}_k=-\pazocal{R}_{\theta}\left(\mathbf{u}_{k}, \boldsymbol{\theta}_{k}\right)-\widehat{\boldsymbol{T}}_{k}^{\prime} \pazocal{R}_{u}\left(\mathbf{u}_{k}, \boldsymbol{\theta}_{k}\right)\\ \mathbf{u}_{k+1} = \mathbf{u}_k + \Delta \hat{\mathbf{u}}_k
$$

Now, $\mathbf u_{k+1}$ can be used to improve the approximation of $\hat{\mathbf U}$, denoted $\hat{\mathbf U}_{k+1}$. Thus, $\boldsymbol \theta_{k+1}$ can now be found from

$$
\left(\boldsymbol{I}-\widehat{\boldsymbol{U}}_{k+1}^{\prime} \widehat{\boldsymbol{T}}_{k}^{\prime}\right) \Delta \hat{\boldsymbol{\theta}}=-\pazocal{R}_{u}\left(\mathbf{u}_{k}, \boldsymbol{\theta}_{k}\right)-\widehat{\boldsymbol{U}}_{k+1}^{\prime} \pazocal{R}_{\theta}\left(\mathbf{u}_{k}, \boldsymbol{\theta}_{k}\right)
$$

and the approximation to the thermal Jacobian updated to $\hat{\mathbf T}_{k+1}$.

It finally remains to precise how are $\hat{\mathbf U}$ and $\hat{\mathbf T}$ approximated. To achieve this consider the intermediate values for $\mathbf u$ and $\boldsymbol \theta$ computed as

$$
\begin{array}{l}
\tilde{\mathbf u}_i = \pazocal U(\boldsymbol \theta_i)\\[5pt]
\tilde{\boldsymbol \theta}_i = \pazocal T(\mathbf u_i)
\end{array}\quad \text{for }i=1,\dots,k,
$$

Firstly, to approximate $\hat{\mathbf U}$, consider the matrices defined as

$$
\mathbf V_U \equiv \{\Delta \boldsymbol \theta_1, \Delta \boldsymbol \theta_2, \dots, \Delta \boldsymbol \theta_k\},\\
\mathbf W_U \equiv \{\Delta \tilde{\mathbf u}_1, \Delta \tilde{\mathbf u}_2, \dots, \Delta\tilde{\mathbf u}_k\}
$$

where the deltas $\Delta \boldsymbol \theta_i = \boldsymbol \theta_{i+1} - \boldsymbol \theta_i$ and $\Delta \tilde{\mathbf u}_i = \tilde{\mathbf u}_{i+1} -\tilde{\mathbf u}_i$ are collected into columns.

A new temperature input delta $\Delta \boldsymbol \theta$ can be approximated by a linear combination of the columns of $\mathbf V_U$ as

$$
\mathbf V_U\mathbf c_U\approx \Delta \boldsymbol\theta
$$

and a corresponding output delta $\Delta \mathbf u$ can then be constructed by using the computed coefficients $\mathbf c_U$to linearly combine the columns of $\mathbf W_U$ as

$$
\Delta\mathbf u = \mathbf W_U\mathbf c_U
$$

The $\mathbf c_F$ are determined from a minimization

$$
\mathbf c_U = \arg\min_{\tilde{\mathbf c} } \|\Delta\boldsymbol \theta - \mathbf V_U\tilde{\mathbf c}\|
$$

which is a least-squares problem, solved in \cite{vierendeels} by the normal equations and in \cite{} using the economy-size $QR$-decomposition. The action of $\hat{\mathbf U}'$ on a vector can then be written as

$$
\hat{\mathbf U}'\Delta\boldsymbol \theta = \mathbf W_U \mathbf c_U
$$

Following \cite{vierrendeels}, the solution for the least-squares problem is

$$
\mathbf c_U = (\mathbf V_U^T\mathbf V_U)^{-1}\mathbf V_U\Delta \boldsymbol \theta
$$

Thus, $\hat{\mathbf U}'$ is found to be

$$
\hat{\mathbf U}' = \mathbf W_U(\mathbf V_U^T \mathbf V_U)^{-1}\mathbf V_U
$$

The process to set up the reduced order model for the Jacobian of the thermal solver $\hat{\mathbf T}$ is analogous to that described for the mechanical solver. Input deltas $\Delta \mathbf u_k \equiv \mathbf u_{k+1} - \mathbf u_k$ and output deltas $\Delta\tilde{\boldsymbol \theta} \equiv \tilde{\boldsymbol \theta}_{k+1} - \tilde{\boldsymbol \theta}_k$ are collected in matrices $\mathbf V_T$ and $\mathbf W_T$. After solving a corresponding least-squares problem to find the optimal coefficients $\mathbf c_T$, the action of $\hat{\mathbf T}'$ on a vector is written as

$$
\hat{\mathbf S}\Delta \mathbf u = \mathbf W_T\mathbf c_T
$$

Box~\ref{} shows this method for one timestep. Initially, a prediction $\boldsymbol \theta_p$ is computed by extrapolation from old timesteps and two block Gauss-Seidel iterations are performed using constant underrelaxation. Then, the initial reduced order models for mechanical and thermal systems are set up and the main coupling iteration loop with computation steps as described above starts.

 Quasi-Newton methods

- Vierendeels et al. (2007)
- Degroote et al. (2008)
- Haelterman et al. (2009)
- Erbst and Düster (2012)
- Gatzhammer (2014)
- Erbts et al. (2015)
- Wendt et al. (2015)

 Newton-Krylov methods

- Michler et al. (2005)
- Küttler and Wall (2009)
- Gatzhammer (2014)
- König et al. (2016)
- Scheufele (2018)

 GMRES

Applying the GMRES to the thermo-mechanical problem, the Krylov-subspace of order $m$ is written as

$$
\pazocal{K}_{m}:=\operatorname{span}\left\{\boldsymbol{s}_{i}-\boldsymbol{s}_{k}\right\}_{i=1}^{m}=\operatorname{span}\left\{\Delta \boldsymbol{s}_{i}\right\}_{i=1}^{m},
$$

---

1. $n = 0$
2. Solve for

 Andersen Acceleration

- Uekermann (2016)

 Generalized Broyden

- Uekermann (2016)

 Filtering

 Multi-Scale

 Mechanical prediction

- Erbts and Düster (2012)
- Erbts et al. (2015)
- Wendt et al. (2015)

A very efficient way to increase the chances of stability and reduce computation time is to predict the optimal initial values at the beginning of every time step. This actually means that we take the results of the mechanical field of the converged solution at the end of the last two or more time increments and predict the new solution by polynomial extrapolation. The predicted values can then be used as an improved initial configuration for the electrical and thermal field. This method is based on polynomial vector extrapolation which is quite easy to implement, and the extra computational input involved is negligible.

Here, the maximum polynomial under consideration is of the order two, i.e., we extrapolate the new solution from the results from the last three time steps. Assuming our time increment will be constant over the whole simulation, the mechanical predictors $\mathbf{U}^{*}$ *for the order $p=1$ and $p=2$ read:*

$$
\begin{array}{l}
p=1: \mathbf{U}_{n+1}^{*}=2 \mathbf{U}_{n}-\mathbf{U}_{n-1} \\
p=2: \mathbf{U}_{n+1}^{*}=3 \mathbf{U}_{n}-3 \mathbf{U}_{n-1}+\mathbf{U}_{n-2}
\end{array}
$$

The subscript $n$ denotes the time step. Where the time increment is adaptive, we refer to [22], since the predictor has to be constructed in an adaptive manner as well.

 Acceleration/Stabilization techniques

 Constant Underrelaxation

- Gatzhammer (2014)

$$
\boldsymbol{s}_{k+1}=(1-\omega) \boldsymbol{s}_{k}+\omega \tilde{\boldsymbol{s}}_{k+1}=\boldsymbol{s}_{k}+\omega \boldsymbol{r}_{k+1}
$$

 Numerical relaxation - Aitken $\Delta^2$-method

- Küttler and Wall (2008)
- Irons and Tuck (1969)
- Joosten et al. (2009)
- Küttler and Wall (2009)
- Erbts et al. (2015)
- Wendt et al. (2015)

To improve the coupling algorithm, we apply Aitken's $\Delta^{2}$-method to accelerate the  convergence of series.

Aitken's delta-squared process is a method of acceleration of convergence, and a particular case of a nonlinear sequence transformation.

In the one-dimensional case, this method resembles the secant method, which can be used to solve nonlinear equations without differentiation.

$\left\{x_{n}\right\}{n \in \mathbb{N}}$ *will converge linearly to $\ell$ if there exists a number $\mu \in(0,1)$ such that*

$$
\lim {n \rightarrow \infty} \frac{\left|x_{n+1}-\ell\right|}{\left|x_{n}-\ell\right|}=\mu .
$$

The formula for the one-dimensional case is

$$
AX_n = \frac{x_n x_{n+2} - x_{n+1}^2}{x_n-2x_{n+1}+ x_{n+2} }
$$

or

$$
AX_n = x_{n+2} - \frac{x_{n+2} - x_{n+1} }{x_n + x_{n+2} - 2 x_{n+1} }(x_{n+1}-x_{n+2})
$$

Aitken's method will accelerate the sequence $x_{n}$ if $\lim_{n \rightarrow \infty} \frac{(A x)_{n}-\ell}{x_{n}-\ell}=0$.
$A$ is not a linear operator, but a constant term drops out, viz: $A[x-\ell]=A x-\ell$, if $\ell$ is a constant. This is clear from the expression of $A x$ in terms of the finite difference operator $\Delta$.
Although the new process does not in general converge quadratically, it can be shown that for a fixed point process, that is, for an iterated function sequence $x_{n+1}=f\left(x_{n}\right)$ for some function $f$, converging to a fixed point, the convergence is quadratic. In this case, the technique is known as Steffensen's method.

Empirically, the $A$-operation eliminates the "most important error term". One can check this by considering a sequence of the form $x_{n}=\ell+a^{n}+b^{n}$, where $0<b<a<1$: The sequence $A x$ will then go to the limit like $b^{n}$ goes to zero.

Geometrically, the graph of an exponential function $f(t)$ that satisfies \(f(n)=x_{n}, f(n+1)=x_{n+1}\) and \(f(n+2)=x_{n+2}\) has an horizontal asymptote at \(\frac{x_{n} x_{n+2}-x_{n+1}^{2}}{x_{n}-2 x_{n+1}+x_{n+2}}\left(\right.\) if \(\left.x_{n}-2 x_{n+1}+x_{n+2} \neq 0\right)\).

If you want to fit \(a e^{b x}+c\) to 3 points, \(\left(x_{i}, y_{i}\right)*{i=1}^{3}\), you want \(y*{i}=a e^{b x_{i}}+c .\)
Subtracting the first two, \(y_{2}-y_{1}=a\left(e^{b x_{2}}-e^{b x_{1}}\right)=a e^{b x_{1}}\left(e^{b\left(x_{2}-x_{1}\right)}-1\right)\) and, similarly, \(y_{3}-y_{2}=a\left(e^{b x_{3}}-e^{b x_{2}}\right)=a e^{b x_{2}}\left(e^{b\left(x_{3}-x_{2}\right)}-1\right) .\)
If \(x_{3}-x_{2}=x_{2}-x_{1}=d\), so the data are equally spaced and \(e^{b\left(x_{2}-x_{1}\right)}-1=e^{b\left(x_{3}-x_{2}\right)}-1\), we can divide these to get
\(\frac{y_{3}-y_{2}}{y_{2}-y_{1}}=e^{b x_{2}-b x_{1}}=e^{b\left(x_{2}-x_{1}\right)}=e^{b d}\)
Taking logs, and letting \(r=\frac{y_{3}-y_{2}}{y_{2}-y_{1}}, \ln (r)=b d\) so \(b=\frac{\ln (r)}{d} .\) From this, \(e^{b}=r^{1 / d} .\)
Note that this requires that \(r>0\), so the points are monotonic.
Then
\[
\begin{aligned}
y_{2}-y_{1} &=a\left(e^{b x_{2}}-e^{b x_{1}}\right) \\
&=a\left(r^{x_{2} / d}-r^{x_{1} / d}\right)
\end{aligned}
\]
so that
\[
a=\frac{y_{2}-y_{1}}{r^{x_{2} / d}-r^{x_{1} / d}}
\]
We also get
\[
a=\frac{y_{3}-y_{2}}{r^{x_{3} / d}-r^{x_{2} / d}}=\frac{y_{3}-y_{1}}{r^{x_{3} / d}-r^{x_{1} / d}}
\]
Finally, \(c=y_{i}-a e^{b x_{i}}\) for any \(i\).

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/def2fba4-2bd4-4a98-8216-4d44881db9ff/Untitled.png)

One can also show that if \(x\) goes to its limit \(\ell\) at a rate strictly greater than \(1, A x\) does not have a better rate of convergence. (In practice, one rarely has e.g. quadratic convergence which would mean over 30 resp. 100 correct decimal places after 5 resp. 7 iterations (starting with 1 correct digit); usually no acceleration is needed in that case.)

Thus, after rearranging,
\[
c=\frac{a b^{*}-a^{*} b}{\left(a-a^{+}\right)-\left(b-b^{+}\right)}
\]
This can be rewritten as
\[
c=\left(1-\beta_{b}\right) b^{*}+\beta_{b} b \quad \text { with } \beta_{b}=\frac{b^{*}-a^{+}}{\left(a-a^{+}\right)-\left(b-b^{+}\right)}
\]
The difference \(b^{*}-a^{*}\) is computationally inconvenient. Anticipating the next iteration step,
\[
d=\left(1-\beta_{c}\right) c^{*}+\beta_{c} c \quad \text { with } \beta_{c}=\frac{c^{*}-b^{*}}{\left(b-b^{*}\right)-\left(c-c^{*}\right)}
\]
a convenient expression for updating the relaxation factor may be found, i.e.
\[
\beta_{c}=\beta_{b}+\left(\beta_{b}-1\right) \frac{c-c^{*}}{\left(b-b^{+}\right)-\left(c-c^{+}\right)}
\]
This expression is used in step 4 of Box 2 .

In practice, \(A x\) converges much faster to the limit than \(x\) does, as demonstrated by the example calculations below. Usually, it is much cheaper to calculate \(A x\) (involving only calculation of differences, one multiplication and one division) than to calculate many more terms of the sequence \(x\). Care must be taken, however, to avoid introducing errors due to insufficient precision when calculating the differences in the numerator and denominator of the expression.

We scrutinize the multi-dimensional case in [38], which forms the basis for the algorithm utilized in many problems, in particular in the field of strongly coupled FSI, see [44] for instance.

The objective is to improve the iteration process so that as few iterations as possible are required to attain a electro-thermo-mechanical problems converged solution.

We now introduce a general vector $\mathbf{D}$, which can either stand for the solution vector of the displacement or the temperature, respectively. The next step is to work out the solution to the current iteration from the outcome of the previous iteration $\mathbf{D}^{(k)}$ plus a new increment $\Delta \mathbf{D}^{(k)}$

$$
\mathbf{D}^{(k+1)}=\mathbf{D}^{(k)}+\Delta \mathbf{D}^{(k)}
$$

The increment reads

$$
\Delta \mathbf{D}^{(k)}=\omega^{(k)}\left(\tilde{\mathbf{D}}^{(k+1)}-\mathbf{D}^{(k)}\right)=\omega^{(k)} \mathbf{R}^{(k)}
$$

with $\omega^{(k)}$ being the relaxation coefficient. From now on, all unmodified values are written with a tilde, like $\tilde{\mathbf{D}}^{(k+1)}$ standing for the unmodified values of the displacement or temperature vector, respectively. The efficiency of this method is mainly due to the relaxation parameter $\omega$. A constant parameter $0<\omega<2$ yields static relaxation (SR), whereas an adaptive parameter results in a dynamic relaxation (DR).

For the dynamic case, we follow [44] where the relaxation coefficient is updated in every iteration cycle as a function of two previous residuals:

$$
\omega^{(k)}=-\omega^{(k-1)} \frac{\left(\mathbf{R}^{(k)}-\mathbf{R}^{(k-1)}\right)^{\mathrm{T}} \mathbf{R}^{(k-1)}}{\left(\mathbf{R}^{(k)}-\mathbf{R}^{(k-1)}\right)^{2}}
$$

Dynamic relaxation is also easy to implement and the additional computational input is acceptable, since only inner vector products have to be performed. Several variations on this method can be found in the literature: A comprehensive study was presented in [47], for instance.

 Vector Extrapolation

$$
\hat{\boldsymbol{s}}_{k+m}=\boldsymbol{s}_{k+1}+\sum_{i=1}^{m-1} \omega_{i}\left(\boldsymbol{s}_{k+i+1}-\boldsymbol{s}_{k+i}\right)=\boldsymbol{s}_{k+1}+\sum_{i=1}^{m-1} \omega_{i} \Delta \boldsymbol{s}_{k+i+1}
$$

$$
\begin{aligned}\Delta \hat{\boldsymbol{s}}_{k+m} &=\sum_{i=0}^{m-1} \gamma_{i} \Delta \boldsymbol{s}_{k+i+1} \\& \sum_{i=0}^{m-1} \gamma_{i}=1\end{aligned}
$$

\[
A \gamma=0
\]
factors \(\gamma_{i}\). The difference between th f the system matrix \(\boldsymbol{A}\). This is done
\[
a_{i j}=\Delta s_{k+i} \cdot \Delta s_{k+j}
\]
extrapolation (MPE) method, as
\[
a_{i j}=\left(\Delta s_{k+i}-\Delta s_{k+i-1}\right) \cdot \Delta s_{k+j}
\]
lation (RRE) method, and as
\[
a_{i j}=\boldsymbol{y}^{i} \cdot \Delta \boldsymbol{s}_{k+j}
\]

 Steepest Descent Relaxation
