\chapter{Strongly coupled methods for coupled fields}

This chapter presents the most common strongly coupled/implicit methods employd for the solution of coupled field problems.
This presentation seeks to provide a literature overview of the available approaches.

% More thorough description of the chapter
Two main ways of realizing a strongly coupled approach to the solution of the coupled problem are presented.
The first focuses on fixed-point solvers, and acceleration/stabilization techniques for them.
The second deals with approaches based on the Newton-Raphson method, with the main problem tackled being an efficient and accurate approximation to the Jacobian.\jvc{In the end rewrite the description of the Chapter}

\section{Equations to be solved}

For the sake of clarity, the discretized equations of the thermo-mechanical problem at the next time instant \(n+1\), which is the focus of the present work, are recovered here
\begin{gather}
    \mathbf M \ddot{\mathbf u}_{n+1} +\mathbf f_u^\text{\;int}(\bm \uptheta_{n+1}, \mathbf u_{n+1})-\mathbf f^\text{\;ext}_{u,n+1}=\mathbf 0, \label{eq:mech_problem} \\
    \mathbf C \dot{\bm \uptheta}_{n+1} +\mathbf f_\theta^\text{\;int}(\bm \uptheta_{n+1}, \mathbf u_{n+1})-\mathbf f^\text{\;ext}_{\theta,n+1}=\mathbf 0, \label{eq:therm_problem}
\end{gather}
where the complete definition of the material incremental discretized thermo-mechanical initial boundary value problem can be found in Chapter~\ref{chapter:thermo_mechanical_problem}.

As only partitioned approaches are being considered, the thermal and mechanical problems are solved separetely, i.e., Equation~\eqref{eq:mech_problem} is solved considering a fixed temperature and Equation~\eqref{eq:therm_problem} is solved assuming a fixed configuration.
To ease the discussion, consider the existence of two functions \(\pazocal U_{n+1}\) and \(\pazocal T_{n+1}\) that represent these solution procedures at timestep \(n+1\).
These so-called mechanical and thermal solvers satisfy
\begin{highlight}[innertopmargin=-5pt]
\begin{gather}
  \pazocal U\colon \mathscr K_{\theta, n+1}\to \mathscr K_{u,n+1},\quad \mathbf u = \pazocal U_{n+1}(\bm \uptheta),\\
  \pazocal T\colon \mathscr K_{u,n+1}\to \mathscr K_{\theta, n+1},\quad \bm \uptheta = \pazocal T_{n+1}(\mathbf u).
\end{gather}
\end{highlight}
See Chapter~\ref{} for detailed information on them.
In the following the subscripts on the solvers will be droped to avoid clutter.

The goal now is to consider functions, built using \(\pazocal U\) and \(\pazocal T\), whose roots are also the solutions to the thermo-mechanical problem (Equations~\eqref{eq:mech_problem} and \eqref{eq:therm_problem}).
Several examples can be provided.
The most appropriate for the current use-case are presented in what follows.
They can be found in \cite{uekermann_partitioned_2016} in the context of fluid-structure interaction.

Consider the residues defined as,
\begin{highlight}
\begin{equation} \label{eq:def_res_jacobi}
  \pazocal R_\text{J}\colon \mathscr K_{u,n+1}\times\mathscr K_{\theta,n+1} \to K_{u,n+1}\times\mathscr K_{\theta,n+1},\quad  \pazocal R_\text{J}(\mathbf u, \bm \uptheta) =
  \left\{\begin{array}{c}
  \mathbf u - \pazocal U(\bm \theta)\\
  \bm \uptheta - \pazocal T(\mathbf u)
  \end{array}\right\},
\end{equation}
\end{highlight}
and
\begin{highlight}
\begin{equation} \label{eq:def_res_gauss_seidel}
  \pazocal R_\text{GS}\colon \mathscr K_{\theta,n+1} \to\mathscr K_{\theta,n+1},\quad \pazocal R_\text{GS}(\bm \uptheta) =
  \bm \uptheta - \pazocal T\circ \pazocal U(\bm \uptheta),
\end{equation}
\end{highlight}
where the subscript "J" stands for Jacobi and the subscript "GS" for Gauss-Seidel.
The reason for this choice of subscripts is made clear in Section~\ref{}.


Since the methods described below for the solution of non-linear systems of equations apply to both functions \(\pazocal R_\mathrm{J}\) and \(\pazocal R_\mathrm{GS}\), a general function denoted as \(\pazocal R\), whose variable is \(\mathbf x\), is considered instead.
As already stated, the solution for the thermo-mechanical problem (Equations~\eqref{eq:mech_problem} and \eqref{eq:therm_problem}) can be abstracted as the solutions of
\begin{equation} \label{eq:abstract_residue_equation}
  \pazocal R(\mathbf x) = 0.
\end{equation}
To obtain simpler expressions in what follows, consider also the function
\begin{equation}
\pazocal S = \mathbf x - \pazocal R(\mathbf x),
\end{equation}
whose fixed point is the solution to the nonlinear system of equation in Equation~\eqref{}.

\jvc{Explain an difference vector/scalar}

\section{A classification scheme for iterative methods}
\jvc{Should we include more classification schemes? Classifiy by order also? And so on?}

Most methods available for the solution of systems of non-linear equations, such as the one in Equation~\eqref{eq:abstract_residue_equation}, are iterative methods.
They can be more precisely defined lettting \(\mathbf x^{k},\mathbf x^{k-1}, \ldots\), whose superscripts correspond to the loop of the iteration method, be approximants to \(\mathbf x_{n+1}\), whose subscript concerns the timestep

To better understand the landscape of available methods to solve nonlinear systems of equations, the iteration functions are classified according to the information they require following the classification scheme by \cite{traub_iterative_1982}.
Let \(\mathbf x^{k+1}\) be determined uniquely by information obtained at \(\mathbf x^{k}, \mathbf \mathbf x^{k-1}, \ldots\), including the derivatives of any order of \(\pazocal R\).
Let the function that maps \(\mathbf x^{k}, \mathbf x^{k-1}, \ldots\) into \(\mathbf x^{k+1}\) be called \(\phi\).
Thus
\begin{highlight}
\begin{equation}
  \mathbf x^{k+1}=\phi\left(\mathbf x^{k},\pazocal R(\mathbf x^{k}), J_\pazocal{R}(\mathbf x^k), \dots, \mathbf x^{k-1},\pazocal R(\mathbf x^{k-1}), J_\pazocal{R}(\mathbf x^{k-1}) \ldots\right),
\end{equation}
\end{highlight}
where \(\phi\) is called an iteration function, and \(J_\pazocal{R}\) is the Jacobian of \(\pazocal R\).
To prevent clutering \(\mathbf x^k\) will stand for its value as well as for the values of \(\pazocal R(\mathbf x^k\), \(J_\pazocal{R}(\mathbf x^k)\) and further derivatives of higher order.
Then \(\phi\) is called a \textit{one-point iteration function}.
Most iteration functions. which have been used for root-finding are one-point iteration functions. The most commonly known examples are the fixed point schemes and Newton's iteration method.

Next let \(\mathbf x^{k+1}\) be determined by new information at \(\mathbf x^{k}\) and reused information at \(\mathbf x^{k-1}, \ldots\).
Thus
\begin{highlight}
  \begin{equation}\label{eq:one_point_iteration_function_with_memory}
    \mathbf x^{k+1}=\phi\left(\mathbf x^{k} ; \mathbf x^{k-1}, \ldots\right) .
  \end{equation}
\end{highlight}
Then \(\phi\) is called a \textit{one-point iteration function with memory}.
The semicolon in Equation~\eqref{eq:one_point_iteration_function_with_memory} separates the point at which new data are used from the points at which old data are reused.
The best-known example of a one-point iteration function with memory is the secant iteration function.

Let \(\mathbf x^{k+1}\) be determined by new information at \(\mathbf x^{k}, \omega_{1}\left(\mathbf x^{k}\right), \ldots\), \(\omega_{i}\left(\mathbf x^{k}\right)\), \(i \geq 1\), where \(\omega_i\) denote operations on \(\mathbf x^k\).
No old information is reused.
Thus
\begin{highlight}
  \begin{equation}
    \mathbf x^{k+1}=\phi\left[\mathbf x^{k}, \omega_{1}\left(\mathbf x^{k}\right), \ldots, \omega_{i}\left(\mathbf x^{k}\right)\right].
  \end{equation}
\end{highlight}
\jvc{Include where each will be discussed.}
Then \(\phi\) is called a \textit{multipoint iterative function}.
Such methods include Aitken-Steffson method.

Finally, let \(\mathbf z_{j}\) represent the quantities \(\mathbf x^{j}, \omega_{1}\left(\mathbf x^{j}\right), \ldots, \omega_{i}\left(\mathbf x^{j}\right)\), \(i \geq 1\).
Let
\begin{highlight}
  \begin{equation} \label{eq:multipoint_iterative_function_with_memory}
  \mathbf x^{k+1}=\phi\left(\mathbf z^{k} ; \mathbf z^{k-1}, \dots \right) .
  \end{equation}
\end{highlight}
Then \(\phi\) is called a \textit{multipoint iterative function with memory}.
The semicolon in Equation~\eqref{eq:multipoint_iterative_function_with_memory} separates the points at which new data are used from the points at which old data are reused.
\jvc{What about Pad√© approximants?}

In the present work the criteria used for the choice of the iterative method used fit neatly into the ones provided by \cite{fang_two_2009} for problems in the context the electronic structure problems.
They are
\begin{enumerate}
  \item The dimensionality of the problem is large.
  \item \(\pazocal R(\mathbf x)\) is continuously differentiable, but the analytic form of its derivative is not readily available, or it is very expensive to compute.
  \item The cost of evaluating \(\pazocal R(\mathbf x)\) is computationally high.
  \item The problem is noisy. In other words, the evaluated function values of \(\pazocal R(\mathbf x)\) usually contain errors.
\end{enumerate}
\jvc{Justify why they fit neatly into our problem?}

As such the methods chosen must minimize the number of calls to \(\pazocal R\), as it is expensive to compute.
The amount of information saved from previous iterations must also be judiciously chosen as the dimensionality of the problem is large and this can lead to memory limitation.
Finally, the analytical for of the derivative \(\pazocal{R}\) is also not available, thus methods that use must be discarded.

\subsection{Predictor}

All iterative procedures considered solve the thermo-mechanical problem at a given timestep \(n+1\).
As the first value approximating \(\mathbf x_{n+1}\), one can employ the converged value of the previous timestep, \(\mathbf x_n\), however a very efficient way to increase the chances of stability and reduce computation time is to predict the optimal initial values at the beginning of every time step \citep{erbts_accelerated_2012, erbts_partitioned_2015, wendt_partitioned_2015}.
The prediction of the new solution by polynomial extrapolation is based on the converged solution of the last two or three timesteps.
This method is based on polynomial vector extrapolation which is quite easy to implement, and the extra computational input involved is negligible.

The maximum polynomial under consideration is of the order two, i.e., the new solution is extrapolated from the results from the last three time steps.
The predictors $\mathbf{x}^{*}$ for the order $p=1$ and $p=2$ polynomials read:
\begin{highlight}[innertopmargin=-5pt]
\begin{gather}
p=1:\quad \mathbf{x}_{n+1}^{*}=2 \mathbf{x}_{n}-\mathbf{x}_{n-1}, \\
p=2:\quad \mathbf{x}_{n+1}^{*}=3 \mathbf{x}_{n}-3 \mathbf{x}_{n-1}+\mathbf{x}_{n-2}.
\end{gather}
\end{highlight}

\subsection{Convergence criteria}

For an iterative method to be useful, there must be reasonable criteria to determine its convergence.
The iteration residual is defined as
\begin{equation}
\mathbf r^{k} = \pazocal R(\mathbf x^{k}),
\end{equation}
and if it is equal to zero then $\mathbf x$ is the solution to the system of nonlinear equations, i.e.,
\begin{equation} \label{eq:residual_definition}
\mathbf r = \pazocal R(\mathbf x) = \mathbf 0,
\end{equation}
and hence, a reasonable convergence measure for the iteration procedure.

The discrete  $L^{2}$-norm can be used to obtain a scalar representative of the vectorial residual \(\mathbf r^{k}=\left(r^{k,1}, \ldots, r^{k,m}\right)^{T}\) as
\begin{equation} \label{eq:absolute_residual_criterion}
\left\|\mathbf{r}^{k}\right\|_{L^{2}}=\sqrt{\sum_{i}\left(r^{k, i}\right)^{2}}.
\end{equation}

Directly using \eqref{eq:absolute_residual_criterion} yields an absolute convergence criterion
\begin{equation}
\left\|\boldsymbol{r} ^{k}\right\|_{L^{2}}<\epsilon_\mathrm{abs}.
\end{equation}
with $\epsilon_\mathrm{abs}>0$ as absolute convergence limit and convergence being obtained when the above condition renders valid.
However, since the absolute value of $r$ can change by orders of magnitude during one simulation, an absolute measure is not appropriate in all situations.
A relative measure solves this problem by setting the residual in relation with the current coupling iterate values as
\begin{equation}
\frac{\left\|\mathbf{r}^{k}\right\|_{L^{2}}}{\left\|\mathbf{x}^{k}\right\|_{L^{2}}}<\epsilon_\mathrm{rel}.
\end{equation}

Since a relative convergence measure can fail to work properly when the coupling iterate values are close to zero and rounding errors come into play, a combination of absolute and relative measure, where the absolute measure takes care of close to zero cases and the relative handles all other cases, is often a good choice.



\section{One-point iteration function}

\jvc{Write small description of the one-point iteration functions we are going to look at}

\subsection{Fixed-point approaches}

The application of the fixed-point method to obtain the roots of \(\pazocal R\) yields
\begin{equation}
  \mathbf x^{k+1} = \pazocal S(\mathbf x^k) = \mathbf x^k - \pazocal R(\mathbf x^k).
\end{equation}
If the particular functions defined on Equations~\eqref{eq:def_res_jacobi} and \eqref{eq:def_res_gauss_seidel} are used, one finds the  two basic Schwarz procedures commonly employed in strongly coupled solution procedures.
They are the additive or block Jacobi and the parallel Scharwz or Gauss-Seidel procedures.
The names originate from domain decomposition, and justify the subscripts employ in Equations~\eqref{eq:def_res_jacobi} and \eqref{eq:def_res_gauss_seidel}.

\subsubsection{Block Jacobi or Schwarz additive}

Applying the fixed-point approach to \(\pazocal R_\mathrm{J}\) (Equation~\eqref{eq:def_res_jacobi}), yields
\begin{align}
  \left\{\mathbf u^{k+1}, \bm\uptheta^{k+1}\right\}^T &= \pazocal S_\mathrm{J}(\mathbf u^k, \bm\uptheta^k)\\
   &= \left\{\mathbf u^k, \bm\uptheta^k\right\}^T - \pazocal R_\mathrm{J}(\mathbf u^k, \bm \uptheta^k),
\end{align}
This is the same as solving both the mechanical (Equation~\eqref{eq:mech_problem}) and the thermal problem (Equation~\eqref{eq:therm_problem}) in parallel.
Such a procedure is said to be Schwarz additive or block Jacobi, refering to the similarities with the procedure for the solution of linear systems of equations with the same name i.e.,
\begin{gather}
\mathbf u^{k+1} = \pazocal U(\bm \uptheta^k),\\
\bm \uptheta^{k+1} = \pazocal T(\mathbf u^k).
\end{gather}

Box~\ref{box:block_jacobi} shows the pseudo-code for the block Jacobi approach.

\begin{framedbox}[htb]
  \caption{Additive Schwarz procedure, also called block Jacobi, for one timestep.}
  \label{box:block_jacobi}
  \begin{center}
    \begin{minipage}{0.9\textwidth}
    \begin{enumerate}[(i)]
    \item \(\mathbf u^0 = \mathbf u_{n}\)
    \item \(\bm \uptheta^0 = \bm \uptheta_n\)
    \item Set fixed-point counter to zero: \(k=0\)
    \item Enter the fixed-point loop
    \begin{enumerate}[(1)]
      \item Solve the mechanical problem at fixed temeperature \(\bm \uptheta^k\): \(\mathbf u^{k+1} = \pazocal U(\bm \uptheta^k)\)
      \item Solve the thermal problem at a fixed configuration \(\mathbf u^k\): \(\bm \uptheta^{k+1} = \pazocal T(\mathbf u^k)\)
      \item If the desired accuracy has not been reached, update \(k=k+1\) and go to step (1).

    \end{enumerate}
    \end{enumerate}
    \end{minipage}
  \end{center}
\end{framedbox}

\subsubsection{Block Gauss-Seidel or Schwarz multiplicative}

Applying the fixed-point approach to \(\pazocal R_\mathrm{GS}\) (Equation~\eqref{eq:def_res_jacobi}), yields
\begin{equation}
  \bm\uptheta^{k+1} = \pazocal S_\mathrm{GS}(\bm\uptheta^k) =  \bm\uptheta^k - \pazocal R_\mathrm{GS}(\bm \uptheta^k).
\end{equation}
Thus, the fields are solved sequentially, where the output of the first solver is used as the input for the second solver.
This the solution procedure is said to be Scharwz multiplicative or block Gauss-Seidel.
\begin{gather}
\mathbf u^{k+1}  = \pazocal U(\bm \uptheta^k),\\
\bm \uptheta^{k+1} = \pazocal T(\mathbf u^{k+1}).
\end{gather}
One of the fields must be chosen as the first and this may be important \citep{joosten_analysis_2009}.
Here, the focus is on the sequence coinciding with the isothermic split, i.e., first the mechanical problem is solved at a fixed temeprature, and then the thermal problem is fixed at a fixed configuration.

Box~\ref{box:block_gauss_seidel} shows the pseudo-code for the block Gauss-Seidel approach.

\begin{framedbox}[htb]
  \caption{Multiplicative Schwarz procedure, also called block Gauss-Seidel, for one timestep.}
  \label{box:block_gauss_seidel}
  \begin{center}
    \begin{minipage}{0.9\textwidth}
    \begin{enumerate}[(i)]
    \item \(\mathbf u^0 = \mathbf u_{n}\)
    \item \(\bm \uptheta^0 = \bm \uptheta_n\)
      \item Set fixed-point counter to zero: \(k=0\)
    \item Enter the fixed-point loop
    \begin{enumerate}[(1)]
      \item Solve the mechanical problem at fixed temeperature \(\bm \uptheta^k\): \(\mathbf u^{k+1} = \pazocal U(\bm \uptheta^k)\)
      \item Solve the thermal problem at a fixed configuration \(\mathbf u^{k+1}\): \(\bm \uptheta^{k+1} = \pazocal T(\mathbf u^{k+1})\)
      \item If the desired accuracy has not been reached, update \(k=k+1\) and go to step (1).

    \end{enumerate}
    \end{enumerate}
    \end{minipage}
  \end{center}
\end{framedbox}

\subsection{Newton's method}

The Newton-Raphson or Newton scheme is a very popular iterative solution procedure for nonlinear systems of equations, which under appropriate conditions converges quadratically.
It can be applied to Equation~\eqref{eq:abstract_residue_equation} yielding
\begin{highlight}[innertopmargin=-5pt]
  \begin{gather}
    J_\pazocal{R}(\mathbf x^k)\Delta \mathbf x^k = - \pazocal R(\mathbf x^k),\\
    \mathbf x^{k+1} = \mathbf x^k + \Delta \mathbf x^k.
  \end{gather}
\end{highlight}

In particular, using \(\pazocal R_\mathrm{GS}\), a few simplifications can be obtain. \jvc{Find the proper citations}
To ease the explanation, consider, a thermal residual operator $\pazocal{R}_{u}(\mathbf{u}, \bm{\uptheta})$ and a mechanical residual operator $\pazocal{R}_{\theta}(\mathbf{u}, \bm{\uptheta})$ defined to be the first and second components in the definition of \(\pazocal R_\mathrm{J}\) (Equation~\eqref{eq:def_res_gauss_seidel}).
Written in full
\begin{gather}
\pazocal{R}_{u}(\mathbf{u}, \bm{\uptheta})=\mathbf{u}-\pazocal{U}(\bm{\uptheta})=0, \\
\pazocal{R}_{\theta}(\mathbf{u}, \bm{\uptheta})=\bm{\uptheta}-\pazocal{T}(\mathbf{u})=0,
\end{gather}

From this, a block Newton iteration can be written as
\begin{equation} \label{eq:block_newton_raphson}
\left[\begin{array}{l}
J_{\pazocal{R}_{u}}\left(\mathbf u^k, \bm{\uptheta}^{k}\right) \\[7pt] J_{\pazocal{R}_{\theta}}\left(\mathbf{u}^{k}, \bm\uptheta^k\right)
\end{array}\right]
\left\{\begin{array}{c}\Delta \mathbf{u}^{k} \\ \Delta \bm{\uptheta}^{k}\end{array}\right\}
=-\left\{\begin{array}{l}\pazocal{R}_{u}\left(\mathbf{u}^{k}, \boldsymbol{\theta}^{k}\right) \\ \pazocal{R}_{\theta}\left(\mathbf{u}^{k}, \boldsymbol{\theta}^{k}\right)\end{array}\right\},
\end{equation}
and the update of the iteration variables reads
\begin{equation}
\left\{\begin{array}{l}
\mathbf{u}^{k+1} \\
\boldsymbol{\theta}^{k+1}
\end{array}\right\}=\left\{\begin{array}{l}
\mathbf{u}^{k} \\
\boldsymbol{\theta}^{k}
\end{array}\right\}+\left\{\begin{array}{c}
\Delta \mathbf{u}^{k} \\
\Delta \boldsymbol{\theta}^{k}
\end{array}\right\}.
\end{equation}


The system of equations in Equation~\eqref{} can be further symplified following \cite{degroote_development_2010} considering the defintions of the mechanical and thermal residuals and taking their derivatives.
It yields
\begin{equation}
\left[\begin{array}{cc}
\mathbf I & J_{\pazocal{U}}\left(\bm{\uptheta}^{k}\right) \\[7pt] J_{\pazocal{T}}\left(\mathbf{u}^{k}\right) & \mathbf I
\end{array}\right]
\left\{\begin{array}{c}\Delta \mathbf{u}^{k} \\ \Delta \bm{\uptheta}^{k}\end{array}\right\}
=-\left\{\begin{array}{l}\pazocal{R}_{u}\left(\mathbf{u}^{k}, \boldsymbol{\theta}^{k}\right) \\ \pazocal{R}_{\theta}\left(\mathbf{u}^{k}, \boldsymbol{\theta}^{k}\right)\end{array}\right\},
\end{equation}
Written in the form of a system of equations
\begin{align}
  \Delta \mathbf u^k + J_\pazocal{U}(\bm \uptheta^k) \Delta \bm\uptheta^k &= - \pazocal R_u(\mathbf u^k, \bm \uptheta^k),\\
  J_\pazocal{T}(\mathbf u^k) \Delta \mathbf u^k + \Delta \bm\uptheta^k &= - \pazocal R_\theta(\mathbf u^k, \bm \uptheta^k).
\end{align}

Solving for \(\Delta \mathbf u^k\) and \(\Delta \bm \uptheta^k\), one finds
\begin{align}
  \left(\mathbf I- J_\pazocal{U}(\bm\uptheta^k)J_\pazocal{T}(\mathbf u^k)\right)\Delta \mathbf u^k &= - \pazocal R_u(\mathbf u^k, \bm \uptheta^k)+J_\pazocal{U}(\bm\uptheta^k)\pazocal R_u(\mathbf u^k, \bm\uptheta^k),\\
  \left(\mathbf I- J_\pazocal{T}(\mathbf u^k)J_\pazocal{U}(\bm\uptheta^k)\right)\Delta \bm\uptheta^k &= - \pazocal R_\theta(\bm \uptheta^k, \mathbf u^k)+J_\pazocal{\theta}(\mathbf u^k)\pazocal R_\theta(\mathbf u^k, \bm\uptheta^k).
\end{align}
Thus, the Jacobians now needed are \(J_\pazocal{U}\) and \(J_\pazocal{T}\).
See Section~\ref{} for the practical application of this.

Every iteration of the Newton scheme involves at least one invocation of the thermal and mechanical solvers when computing $\pazocal{R}\left(\mathbf{u}^{k}\right)$ or both $\pazocal{R}_{u}\left(\mathbf{u}^{k}, \boldsymbol{\theta}^{k}\right)$ and $\pazocal{R}_{\theta}\left(\mathbf{u}^{k}, \boldsymbol{\theta}^{k}\right)$.

The critical point for black box equation coupling is how to obtain the derivative information in the Jacobi matrices.
Some of the methods presented in the following find approximations for the required Jacobian times vector products in different ways.


\section{One-point iteration function with memory}

\jvc{Description of the methods}

\subsubsection{Aitken relaxation}

\citep{irons_version_1969, kuttler_fixed-point_2008, joosten_analysis_2009, kuttler_vector_2009, erbts_partitioned_2015, wendt_partitioned_2015}


The so-called Aitken \(\Delta^2\) relaxation method was introduced by \cite{irons_version_1969}, as a modified Aitken \(\Delta^2\) that doesn't require the computation of the function twice per iteration as in the original method.
It also provides a generalization to the vector case.
In the one-dimensional case, this method resembles the secant method applied to the fixed point problem, which can be used to solve nonlinear equations without differentiation.
Calling it an Aitken method is perhaps a misnommer since in the Aitken-Steffson method, the values of the function are computed twice prer iterartion.
It is more closely related to secant methods, where values from previous iterations.
This version of Aitken's \(\Delta^2\) method provides a dynamic underrelaxation, which can be used to improve the convergence/stability properties of coupling algorithm.

Assume that \(f\) is the function whose fixed point is sought.
The linear interpolation between two points already known of the function, \((a, f(a))\) and \((b, f(b)\) is
\begin{equation}
  y = \frac{f(b)-f(a)}{b-a}(x-a) - f(a).
\end{equation}
The fixed point of this approximation is
\begin{equation}
  c = \frac{f(b)-f(a)}{b-a}(x-c) - f(a).
\end{equation}
Thus, after rearranging,
\begin{equation}
c=\frac{a f(b)- b f(a)}{\left(a-f(a)\right)-\left(b-f(b)\right)}
\end{equation}
This can be rewritten as
\begin{equation}
c=\left(1-\omega_{b}\right) f(b)+\omega_{b} b \quad \text { with } \omega_{b}=\frac{f(b)-f(a)}{\left(a-f(a)\right)-\left(b-f(b)\right)}
\end{equation}

The difference \(f(b)-f(a)\) is computationally inconvenient.\jvc{Why?} 
Anticipating the next iteration step,
\begin{equation}
d=\left(1-\omega_{c}\right) f(c)+\omega_{c} c \quad \text { with } \omega_{c}=\frac{f(c)-f(b)}{\left(b-f(b)\right)-\left(c-f(c)\right)}
\end{equation}
a convenient expression for updating the relaxation factor may be found, i.e.
\begin{equation}
\omega_{c}=\omega_{c}+\left(\omega_{b}-1\right) \frac{c-f(c)}{\left(b-f(b)\right)-\left(c-f(c)\right)}.
\end{equation}

Now, for the vector case, the next step is to work out the solution to the current iteration from the outcome of the previous iteration $\mathbf{x}^{(k)}$ plus a new increment $\Delta \mathbf{x}^{(k)}$
\begin{equation}
\mathbf{x}^{(k+1)}=\mathbf{x}^{(k)}+\Delta \mathbf{x}^{(k)}.
\end{equation}
The increment reads
\begin{equation}
\Delta \mathbf{x}^{(k)}=\omega^{(k)}\left(\pazocal S(\mathbf{x}^{(k)})-\mathbf{x}^{(k)}\right)=\omega^{(k)} \mathbf r^{(k)}.
\end{equation}
with $\omega^{(k)}$ being the relaxation coefficient.
This coefficient is updated in every iteration cycle as a function of two previous residuals
\begin{highlight}
  \begin{equation}
    \omega^{(k)}=-\omega^{(k-1)} \frac{\left(\mathbf{r}^{(k)}-\mathbf{r}^{(k-1)}\right)^{\mathrm{T}} \mathbf{r}^{(k-1)}}{\left(\mathbf{r}^{(k)}-\mathbf{r}^{(k-1)}\right)^{2}}.
  \end{equation}
\end{highlight}

Dynamic relaxation is also easy to implement and the additional computational input is acceptable, since only inner vector products have to be performed. 

\subsection{Broyden's method and Broyden's family}

In quasi-Newton methods the Jacobian is updated in each iteration using a rank-one update.
Standard quasi-Newton methods require the updated \(J_{k+1}\) to satisfy the following secant condition
\begin{equation} \label{eq:secant_condition}
J_{k+1} \Delta x_{k}=\Delta f_{k},
\end{equation}
where \(\Delta f_{k}:=f\left(x_{k+1}\right)-f\left(x_{k}\right)\).
Furthermore, another common requirement is the following so-called no-change condition:
\begin{equation} \label{eq:no_change_condition}
J_{k+1} q=J_{k} q \quad \forall q \text { such that } q^{\mathrm{T}} \Delta x_{k}=0,
\end{equation}
which stipulates that there be no new information from \(J_{k}\) to \(J_{k+1}\) along any direction \(q\) orthogonal to \(\Delta x_{k}\).

\cite{broyden} developed a method satisfying both secant condition (Equation~\eqref{eq:secant_condition}) and the no-change condition (Equation~\eqref{eq:no_change_condition}). 
By simply imposing these conditions he arrived at the update formula
\begin{highlight}
\begin{equation} \label{eq:good_update_broyden}
J_{k+1}=J_{k}+\left(\Delta f_{k}-J_{k} \Delta x_{k}\right) \frac{\Delta x_{k}^{\mathrm{T}}}{\Delta x_{k}^{\mathrm{T}} \Delta x_{k}}.
\end{equation}
\end{highlight}

Matrix \(J_{k+1}\) in (6) is the unique matrix satisfying both conditions (4) and (5). Dennis and Mor√© [13] showed that the Broyden update can also be obtained by minimizing \(E\left(J_{k+1}\right)=\left\|J_{k+1}-J_{k}\right\|_{F}^{2}\) with respect to terms of \(J_{k+1}\), subject to the secant condition (4).

It may seem at first that Broyden's first method can be expensive since computing the quasi-Newton step \(\Delta x_{k}\) requires solving a linear system at each iteration. 
However, note that, typically, the approximate Jacobian is a small rank modification of a diagonal matrix (or a matrix that is easy to invert); hence, the cost to obtain this solution is actually not too high as long as the number of steps is not too large.

An alternative is Broyden's second method that approximates the inverse Jacobian instead of the Jacobian itself. 
\(G_{k}\) is used to denote the estimated inverse Jacobian at the \(k\) th iteration. 
The secant condition (Equation~\eqref{eq:secant_condition}) now reads
\begin{equation} \label{eq:inverse_secant_cond}
G_{k+1} \Delta f_{k}=\Delta x_{k}
\end{equation}
By minimizing \(E\left(G_{k+1}\right)=\left\|G_{k+1}-G_{k}\right\|_{F}^{2}\) with respect to \(G_{k+1}\) subject to Equation~\eqref{eq:inverse_secant_cond}, the following update formula is found for the inverse Jacobian
\begin{highlight}
  \begin{equation} \label{eq:bad_update_broyden}
  G_{k+1}=G_{k}+\left(\Delta x_{k}-G_{k} \Delta f_{k}\right) \frac{\Delta f_{k}^{\mathrm{T}}}{\Delta f_{k}^{\mathrm{T}} \Delta f_{k}}
  \end{equation}
\end{highlight}
which is also the only update satisfying both the secant condition (Equation~\eqref{eq:inverse_secant_cond}) and the no-change condition for the inverse Jacobian
\begin{equation}
  G_{k} q=G_{k+1} q \quad \forall q \text { such that } q^{\mathrm{T}} \Delta f_{k}=0.
\end{equation}
The update formula in Equation~\eqref{eq:good_update_broyden} can also be obtained in terms of \(G_{k} \equiv J_{k}^{-1}\) by applying the Sherman-Morrison formula
\begin{equation}
G_{k+1}=G_{k}+\left(\Delta x_{k}-G_{k} \Delta f_{k}\right) \frac{\Delta x_{k}^{\mathrm{T}} G_{k}}{\Delta x_{k}^{\mathrm{T}} G_{k} \Delta f_{k}}
\end{equation}
This shows, as was explained earlier, that to solve the Jacobian system associated with Broyden's first approach can be reduced to a set of update operations that are not more costly than those required by the second update. 
Note, however, that the above formula requires the inverse of the initial Jacobian.

From Equation~\eqref{eq:good_update_broyden} and Equation~\eqref{eq:bad_update_broyden} it is possible to define Broyden's family of updates, in which an update formula takes the general form
\begin{equation}
G_{k+1}=G_{k}+\left(\Delta x_{k}-G_{k} \Delta f_{k}\right) v_{k}^{\mathrm{T}}
\end{equation}
where \(v_{k}^{\mathrm{T}} \Delta f_{k}=1\) so that the secant condition (7) holds [6]. Note that the secant condition \eqref{eq:inverse_secant_cond} is equivalent to condition \eqref{eq:secant_condition}. 
The pseudo-code of Broyden's two methods is given in Box~\ref{}.
Some authors called Broyden's first method as Broyden's good update, and Broyden's second method as Broyden's bad update. 
These are two particular members in Broyden's family.

\subsubsection{Steepest Descent Relaxation}

In [97], a steepest descent based relaxation method is investigated. The goal is to find an optimal value \(\omega_{k}\) for every iteration \(k\) and to use it in the relaxation (2.112). A convex scalar merit function \(\phi(\boldsymbol{s})\) is assumed to exist, to be minimal at the solution \(\boldsymbol{s}^{n+1}\), and to be sufficiently smooth. This merit function is not defined in [97], only its derivative is used within the algorithm which is defined to be the coupling iteration residual
\[
\phi^{\prime}\left(\boldsymbol{s}_{k}\right):=\boldsymbol{r}_{k+1},
\]

such that \(\phi^{\prime}\left(s^{n+1}\right)=\mathbf{0}\). Using the merit function, an optimal relaxation factor can be found by
\[
\omega_{k}=\underset{\omega}{\arg \min } \phi\left(\boldsymbol{s}_{k}+\omega \boldsymbol{r}_{k+1}\right)
\]
and a sufficient condition for the optimal \(\omega_{k}\) is given by
\[
\frac{d \phi}{d \omega_{k}}=\left(\phi^{\prime}\left(\boldsymbol{s}_{k}+\omega_{k} \boldsymbol{r}_{k+1}\right)\right)^{T} \boldsymbol{r}_{k+1} \stackrel{!}{=} \mathbf{0} .
\]
An approximation for the optimal relaxation factor can be obtained from a truncated Taylor expansion of its first derivative
\[
\phi^{\prime}\left(\boldsymbol{s}_{k}+\omega_{k} \boldsymbol{r}_{k+1}\right) \approx \underbrace{\phi^{\prime}\left(\boldsymbol{s}_{k}\right)}_{=\boldsymbol{r}_{k+1}}+\omega_{k} \phi^{\prime \prime}\left(\boldsymbol{s}_{k}\right) \boldsymbol{r}_{k+1},
\]
where \(\phi^{\prime \prime}\left(s_{k}\right)\) is the symmetric matrix of second order partial derivatives of \(\phi\left(s_{k}\right)\) or the Jacobian of \(r_{k}\) with respect to \(s_{k}{ }^{5}\) Transposing (2.133) and multiplying it from the right with \(r_{k+1}\), the optimal relaxation factor can be obtained by
\[
\omega_{k}=-\frac{\left(\boldsymbol{r}_{k+1}\right)^{T} \boldsymbol{r}^{k+1}}{\left(\boldsymbol{r}^{k+1}\right)^{T} J_{\boldsymbol{r}_{k}}(\boldsymbol{s}) \boldsymbol{r}_{k+1}} .
\]
The remaining problem is how to determine the interface Jacobian \(J_{r_{k}}(s)\), since it is not accessible when black box solvers are used. Two approximations for the matrix vector product \(J_{\boldsymbol{r}_{k}}(s) \boldsymbol{r}_{k+1}\) are proposed in [97]. The first approximation is obtained by using a finite difference approach as
\[
J_{\boldsymbol{r}_{k}}(\boldsymbol{s}) \boldsymbol{y} \approx \frac{\boldsymbol{S} \circ \boldsymbol{F}\left(\boldsymbol{s}_{k}+\delta \boldsymbol{y}\right)-\boldsymbol{s}_{k}-\delta \boldsymbol{y}-\boldsymbol{r}_{k+1}}{\delta},
\]
with
\[
\delta=\frac{\lambda\left(\lambda+\left\|\boldsymbol{s}_{k}\right\|_{L_{2}}\right)}{\left\|\boldsymbol{r}_{k+1}\right\|_{L_{2}}}
\]
and \(\lambda\) chosen small enough \({ }^{6}\). The evaluation of (2.135) needs one more call of fluid and structure solvers per coupling iteration. To decrease the computational costs for the extra evaluation, it is proposed to reduce the accuracy within the field solvers. The second approximation uses approximated fluid derivatives and exact structure derivatives, which are not available for black box coupling and, thus, this alternative is not considered here.

\subsection{Multi-secant methods}

The multi-secant methods provide an approximation to the Jacobian in Equation~\eqref{} or Equation~\eqref{} using information from previous iterations.
The following exposing follows closely \cite{fang_two_2009}.

\subsubsection{Generalized Broyden}

Inspired by the work of Vanderbilt and Louie [11], Eyert [9] proposed a generalized Broyden's method with a flexible rank of update on the inverse Jacobian, satisfying a set of \(m\) secant equations
\begin{equation} \label{eq:multi_secant_eqs}
  G_{k} \Delta f_{i}=\Delta x_{i} \quad \text { for } i=k-m, \ldots, k-1
\end{equation}
where it is assumed \(\Delta f_{k-m}, \ldots, \Delta f_{k-1}\) are linearly independent and \(m \leqslant n\). 
Aggregating Equations~\eqref{eq:multi_secant_eqs} in matrix form, they can be rewriten it as
\begin{equation} \label{eq:multi_secant_eqs_mat}
  G_{k} \mathscr{F}_{k}=\mathscr{X}_{k}.
\end{equation}
where
\begin{equation}
\mathscr{F}_{k}=\left[\Delta f_{k-m} \cdots \Delta f_{k-1}\right], \quad \mathscr{X}_{k}=\left[\Delta x_{k-m} \cdots \Delta x_{k-1}\right] \in \mathbb{R}^{n \times m}
\end{equation}
The no-change condition corresponding to \((9)\) is
\begin{equation}
  \left(G_{k}-G_{k-m}\right) q=0
\end{equation}
for all \(q\) orthogonal to the subspace spanned by \(\Delta f_{k-m}, \ldots, \Delta f_{k-1}\), the columns of \(\mathscr{F}_{k}\). 
This means that the null space of \(G_{k}-G_{k-m}\) is the orthogonal of \(\mathscr{F}_{k}\). 
Since \(\operatorname{Null}(X)^{\perp}=\operatorname{Range}\left(X^{\mathrm{T}}\right)\) this is equivalent to the condition that \(\left(G_{k}-G_{k-m}\right)^{\mathrm{T}}=\mathscr{F}_{k} Z^{\mathrm{T}}\) for a certain unknown matrix \(Z\).
 This matrix \(Z\) can now be obtained from condition \eqref{eq:multi_secant_eqs_mat}. Multiplying \(G_{k}-G_{k-m}=Z \mathscr{F}_{k}^{\mathrm{T}}\) to the right by \(\mathscr{F}_{k}\), we get
\begin{equation}
  \left(G_{k}-G_{k-m}\right) \mathscr{F}_{k}=Z \mathscr{F}_{k}^{\mathrm{T}} \mathscr{F}_{k} \Rightarrow Z=\left(\mathscr{X}_{k}-G_{k-m} \mathscr{F}_{k}\right)\left(\mathscr{F}_{k}^{\mathrm{T}} \mathscr{F}_{k}\right)^{-1}
\end{equation}
where we assume that \(\mathscr{F}_{k}\) has full column rank. 
In the end, this yields
\begin{equation}
  G_{k}=G_{k-m}+\left(\mathscr{X}_{k}-G_{k-m} \mathscr{F}_{k}\right)\left(\mathscr{F}_{k}^{\mathrm{T}} \mathscr{F}_{k}\right)^{-1} \mathscr{F}_{k}^{\mathrm{T}}
\end{equation}
a rank-\(m\) update formula. 
Note that \(\operatorname{rank}\left(\mathscr{F}_{k}\right)=m\).
The update formula for \(x_{k+1}\) is obtained by substituting (16) into (3)
\begin{align}
x_{k+1} &=x_{k}-G_{k} f_{k} \\
&=x_{k}-G_{k-m} f_{k}-\left(\mathscr{X}_{k}-G_{k-m} \mathscr{F}_{k}\right)\left(\mathscr{F}_{k}^{\mathrm{T}} \mathscr{F}_{k}\right)^{-1} \mathscr{F}_{k}^{\mathrm{T}} f_{k} \\
&=x_{k}-G_{k-m} f_{k}-\left(\mathscr{X}_{k}-G_{k-m} \mathscr{F}_{k}\right) \gamma_{k}
\end{align}
where the column vector \(\gamma_{k}\) is obtained by solving the normal equations \(\left(\mathscr{F}_{k}^{\mathrm{T}} \mathscr{F}_{k}\right) \gamma_{k}=\mathscr{F}_{k}^{\mathrm{T}} f_{k}\), which is equivalent to solving the least squares problem
\begin{equation}
  \min _{\gamma}\left\|\mathscr{F}_{k} \gamma-f_{k}\right\|_{2}
\end{equation}
Note that in (17), if \(\mathscr{F}_{k}\) is square and of full rank, then for any \(G_{k-m}\),
\begin{equation}
  x_{k+1}=x_{k}-\mathscr{x}_{k} \mathscr{F}_{k}^{-1} f_{k}
\end{equation}
the same form as that in the standard secant method (see, e.g. [20]).

\subsubsection{Anderson mixing}

Consider a procedure for solving a large nonlinear system of equations \(f(x)=0\) by an iterative process. 
The most recent iterates are denoted by \(x_{k-m}, \ldots, x_{k} \in \mathbb{R}^{n}\) and the corresponding outputs \(f_{k-m}, \ldots, f_{k} \in \mathbb{R}^{n}\). 
Assuming evaluating \(f(x)\) is expensive and no explicit analytic form of \(f(x)\) is available, the challenge is to determine the next estimate \(x_{k+1}\) that approximates the solution to \(f(x)=0\) without additional evaluations of \(f(x)\).
The Anderson mixing scheme [5] takes the latest \(m\) steps into account \({ }^{\ddagger}\) 
\begin{align}
  \bar{x}_{k}=x_{k}-\sum_{i=k-m}^{k-1} \gamma_{i}^{(k)} \Delta x_{i}=x_{k}-\mathscr{X}_{k} \gamma_{k} \\
  \bar{f}_{k}=f_{k}-\sum_{i=k-m}^{k-1} \gamma_{i}^{(k)} \Delta f_{i}=f_{k}-\mathscr{F}_{k} \gamma_{k}
\end{align}
where \(\Delta x_{i}=x_{i+1}-x_{i}\) and \(\Delta f_{i}=f_{i+1}-f_{i}\), \(\mathscr{X}_{k}=\left[\Delta x_{k-m} \cdots \Delta x_{k-1}\right]\), \(\mathscr{F}_{k}=\left[\Delta f_{k-m} \cdots \Delta f_{k-1}\right]\), and \(\gamma_{k}=\left[\gamma_{k-m}^{(k)} \cdots \gamma_{k-1}^{(k)}\right]^{\mathrm{T}}\). Expressing the equations in the form \(\bar{x}_{k}=\sum_{j=k-m}^{k} w_{j} x_{j}\) and \(\bar{f}_{k}=\) \(\sum_{j=k-m}^{k} w_{j} f_{j}\), we find that \(\sum_{j=k-m}^{k} w_{j}=1\). In other words, \(\bar{x}_{k}\) and \(\bar{f}_{k}\) are weighted averages of \(x_{k-m}, \ldots, x_{k}\) and \(f_{k-m}, \ldots, f_{k}\), respectively.
The arguments \(\gamma_{i}=\left[\gamma_{k-m}^{(k)} \cdots \gamma_{k-1}^{(k)}\right]^{\mathrm{T}}\) are determined by minimizing
\begin{equation}
E\left(\gamma_{k}\right)=\left\langle\bar{f}_{k}, \bar{f}_{k}\right\rangle=\left\|f_{k}-\mathscr{F}_{k} \gamma_{k}\right\|_{2}^{2}
\end{equation}
whose solution can (but should not in practice) be obtained by solving the normal equations
\begin{equation}
\left(\mathscr{F}_{k}^{\mathrm{T}} \mathscr{F}_{k}\right) \gamma_{k}=\mathscr{F}_{k}^{\mathrm{T}} f_{k}
\end{equation}
Combining (20), (21), and (23), we obtain
\begin{align}
x_{k+1} &=\bar{x}_{k}+\beta \bar{f}_{k} \\
&=x_{k}+\beta f_{k}-\left(\mathscr{X}_{k}+\beta \mathscr{F}_{k}\right) \gamma_{k} \\
&=x_{k}+\beta f_{k}-\left(\mathscr{X}_{k}+\beta \mathscr{F}_{k}\right)\left(\mathscr{F}_{k}^{\mathrm{T}} \mathscr{F}_{k}\right)^{-1} \mathscr{F}_{k}^{\mathrm{T}} f_{k}
\end{align}
where \(\beta\) is the preset mixing parameter \({ }^{\S}\) and \(\mathscr{F}_{k}^{\mathrm{T}} \mathscr{F}_{k}\) is assumed to be nonsingular. 
In particular, if no previous iterate is taken into account (i.e. \(m=0\) ), then (24) reads
\begin{equation}
  x_{k+1}=x_{k}+\beta f_{k}
\end{equation}
This scheme is referred to as simple mixing.
The update formula (24) is the same as (17) by setting \(G_{k-m}=-\beta I\). In this respect Anderson mixing implicitly forms an approximate inverse Jacobian \(G_{k}\) that minimizes \(\left\|G_{k}+\beta I\right\|_{F}\) subject to (13). In the context of mixing, generalized Broyden's second method is equivalent to Anderson mixing. This equivalence relation was shown by Eyert [9]. Note that if \(\mathscr{F}_{k}\) is square and nonsingular, then (24) matches formula (19) of the standard secant method.

\subsubsection{Generalized Broyden's family}

\subsubsection{Anderson's family}

\subsubsection{The Broyden-like class}

\subsubsection{Quasi-Newton Method}

An interface quasi-Newton method based on Equation~\eqref{eq:residual_fixed_point} and Equation~\eqref{eq:newton_raphson} is presented in \cite{degroote_joris_development_2010} for FSI.
The method is called interface quasi Newton with approximation of the inverse of the interface Jacobian matrix by least squares (IQN-ILS).
Its origin, the IBQN-LS method presented in \cite{vierendeels_implicit_2007}, is derived from the block Newton method Equation~\eqref{eq:block_newton_raphson} and uses two reduced order models for fluid and structure to approximate their interface Jacobians directly.
In contrast, the IQN-ILS method provides one reduced order model for the inverse of the overall interface Jacobian matrix of the Newton system (Equation~\eqref{eq:newton_raphson}) applied to the right-hand side vector.
In the following, the method is adapted to the thermo-mechanical problem under analysis and are based on the description of the method in \cite{gatzhammer_efficient_2014}.
It is henceforth denominated as quasi Newton with approximation of the inverse of the Jacobian matrix by least squares (QN-LS).

As already stated, the Jacobian in Equation~\eqref{eq:newton_raphson} is approximated using a reduced order model.
Since computing \(\pazocal R\) is expensive the estimate for the inverse of the Jacobian tries to take the most advantage from the values already computed so far during the iterative procedure.
Appropriate input and output deltas are used to construct it.
In the residual formulation (Equation~\eqref{eq:residual_fixed_point}) of the chained solvers, the input is given by the temperature values $\bm \uptheta$ and the output by the residual $\mathbf{r}$.
Input and output deltas are defined by
\begin{gather}
\Delta \mathbf{r}^{k}=\mathbf{r}^{k+1}-\mathbf{r}^{k} \\
\Delta \tilde{\bm{\uptheta}}^{k}=\tilde{\bm{\uptheta}}^{k+1}-\tilde{\bm{\uptheta}}^{k}
\end{gather}
i.e., not the actual input deltas $\Delta \bm{\uptheta}$ are gathered, but the intermediate chained solver outputs $\tilde{\bm{\uptheta}}$ are used to construct the $\Delta \tilde{\bm{\uptheta}}$.
This enables the approximation the inverse of the Jacobian applied to the right-hand side vector of the Newton system, as is shown below.

For convenience consider the matrices
\begin{align}
\mathbf{V}_{k} &=\left(\Delta \mathbf{r}^{k}, \Delta \mathbf{r}^{k-1}, \ldots, \Delta \mathbf{r}^{1}, \mathbf{V}^{n-1}, \ldots, \mathbf{V}^{n-l}\right)\label{eq:matrix_delta_res} \\
\mathbf{W}{k} &=\left(\Delta \tilde{\bm{\uptheta}}^{k}, \Delta \tilde{\bm{\uptheta}}^{k-1}, \ldots, \Delta \tilde{\bm{\uptheta}}^{1}, \mathbf{W}^{n-1}, \ldots, \mathbf{W}^{n-l}\right) \label{eq:matrix_delta_theta}
\end{align}
with inversed ordering of the columns compared to the original IBQN-LS method (see Section~\ref{}).
The matrices \(\mathbf{V}^{n-1}, \ldots, \mathbf{V}^{n-l}\) and \(\mathbf{W}^{n-1}, \ldots, \mathbf{W}^{n-l}\) represent the (optional) reuse of columns from \(l\) old timesteps.
Reusing old timestep information can improve the approximation of the inverse of the Jacobian, particularly in the first iterations of a timestep.

% The quantity to be approximated is
% \begin{equation}
%   \hat{J}_{\pazocal R}^{-1}(\bm\uptheta_k)(-\pazocal R(\bm\uptheta_k)),
% \end{equation}
% i.e., the solution to the system of equations in Equation~\eqref{eq:newton_raphson}.
% Employing a slightly different notation, it can be written as
% \begin{equation}
%   \hat{J}_{\pazocal R}^{-1}(\bm\uptheta_k)\Delta\mathbf r^k,
% \end{equation}
% assuming \(\mathbf r^{k+1}=\mathbf 0\) and thus \(\Delta \mathbf r^k = \mathbf r^{k+1} - \mathbf r^k = \mathbf 0 -\mathbf r^k\).
%
% For every pair \(\Delta \mathbf r^i\) and \(\Delta \bm \uptheta^i\), there is a corresponding estimate for the inverse of the Jacobian at \(\bm \uptheta^i\), which can be found from
% \begin{equation}
%   \hat{J}^{-1}_\pazocal{R}(\bm\uptheta^i) \Delta \mathbf r^i = \Delta \bm\uptheta^i.
% \end{equation}
%
% Thus, the inverse of the Jacobian at \(\bm \uptheta^k\) is to be approximated by a linear combination of the available estimates for the Jacobian.
% To avoid solving the linear system, the vector \(\hat{J}^{-1}_\pazocal{R}(\uptheta^k)\Delta \mathbf r^k\) is considered instead, yielding
% \begin{equation}
%   \hat{J}^{-1}_\pazocal{R}(\bm\uptheta^k) \Delta \mathbf r^k = \sum_{i=1}^k c_i \hat{J}^{-1}_\pazocal{R}(\uptheta^i) \Delta \mathbf r^i.
% \end{equation}
% From Equation~\eqref{} we can substitute and find
% \begin{equation}
%   \sum_{i=1}^k c_i \hat{J}^{-1}_\pazocal{R}(\uptheta^i)\Delta \mathbf r^i = \sum_{i=1}  c_i \Delta \bm\uptheta^i = \mathbf W_K\mathbf c,
% \end{equation}
% thus
% \begin{equation}
%   \hat{J}^{-1}_\pazocal{R}(\bm\uptheta^k) \Delta \mathbf r^k = \mathbf W_k \mathbf c.
% \end{equation}
%
% An appropriate choice for \(\mathbf c\) will be one that leads to a good approximation of \(J^{-1}_\pazocal{R}(\bm \uptheta^k)\).
% Given that each approximation to the inverse of the Jacobian is related to a \(\Delta \mathbf r\)), and assuming that it is a good aroximation whatever the \(\Delta \mathbf r\) \(\Delta \mathbf r^k\) by the available \(\Delta \mathbf r^i\), e.g., the minimization of


A linear combination of residual deltas from \(V_{k}\) is used to approximate
\begin{equation}
\Delta \mathbf r \approx \mathbf V_{k} \mathbf c
\end{equation}
which is defined to be \(\Delta \mathbf{r}=\mathbf{0}-\mathbf{r}^{k+1}\), to get a new residual \(\mathbf{r}^{k+2}\) that is zero.
The desired linear combination is found by the least-squares solution
\begin{equation}
\mathbf{c}=\underset{\bar{\mathbf c}}{\arg \min }\left\|\Delta \mathbf{r}-\mathbf{V}_{k} \tilde{\mathbf{c}}\right\|
\end{equation}
which can be computed, e.g., by an economy size \(Q R\)-decomposition.
Here, a possible problem is that the columns of \(\mathbf{V}_{k}\) can become linearly dependent such that a column removal becomes necessary.
Adding new columns as first columns, as in Equations~\eqref{eq:matrix_delta_res} and \eqref{eq:matrix_delta_theta}, enables to detect and remove older columns as linearly dependent during the least-squares solution.
This is particularly important when columns are reused from old timesteps since, then, the newer columns better represent the current state of the residual operator (Equation~\eqref{eq:residual_fixed_point}).

A \(\Delta \tilde{\bm{\uptheta}}\) corresponding to a \(\Delta \mathbf{r}\) is then constructed by using the coefficients \(\mathbf{c}\) in combination with the matrix \(\mathbf{W}_{k}\) as
\begin{equation} \label{eq:linear_approx_output}
\Delta \tilde{\bm{\uptheta}}=\mathbf{W}_{k} \mathbf{c}.
\end{equation}
To construct an approximation for the inverse of the interface Jacobian, the residual formulation (Equation~\eqref{eq:residual_definition}) using \(\Delta \mathbf{r}\), \(\Delta \tilde{\bm{\uptheta}}\), and \(\Delta \bm{\uptheta}\), reading
\begin{equation} \label{eq:relation_res_out}
  \Delta \mathbf r = \Delta \tilde{\bm\uptheta} - \Delta\bm\uptheta.
\end{equation}
Inserting Equation~\eqref{eq:linear_approx_output} into Equation~\eqref{eq:relation_res_out} yields the desired approximation for the inverse of the residue Jacobian \(\hat{J}^{-1}_\pazocal{R}(\bm\uptheta)\) applied to the right-hand side residual delta
\begin{equation}
  \Delta \bm\uptheta^{k+1} = - \hat{J}^{-1}_\pazocal{R}(\bm\uptheta^k)\mathbf r^{k+1} \equiv \mathbf W_k \mathbf c + \mathbf r^{k+1}.
\end{equation}
Box~\eqref{box:quasi_newton} shows the QN-LS method for one timestep of the couple simulation.
When no delta columns are available yet, constant relaxation is used once to ensure stability.


\subsection{Block Quasi-Newton Method}

Linear reduced order models for the fluid solver and the structure solver are used in [172] to speed up the convergence of the FSI coupling iterations. As important feature, the reduced order models are set up from solver input and output deltas (or sensitivities) during the coupling iterations. The resulting method for two black box solvers is called interface block quasi-Newton method with least-squares approximation (IBQN-LS) in [42].

It application to a thermo-mechanical solver is detailed in what follows based on the version of the method for FSI presented in \cite{gatzhammer}.

In this method, the block Newton system (Equation~\eqref{}) is approximated by

$$
\left[\begin{array}{cc}
\widehat{\boldsymbol{U}^{\prime}} & -\boldsymbol{I} \\
-\boldsymbol{I} & \widehat{\boldsymbol{T}^{\prime}}
\end{array}\right]\left\{\begin{array}{c}
\Delta \hat{\mathbf{u}}_k \\
\Delta \hat{\boldsymbol{\theta}}_k
\end{array}\right\}=\left\{\begin{array}{l}
\pazocal{R}_{u}(\mathbf{u}_k, \boldsymbol{\theta}_k) \\
\pazocal{R}_{\theta}(\mathbf{u}_k, \boldsymbol{\theta}_k)
\end{array}\right\}
$$

where $\widehat{\boldsymbol{U}^{\prime}}$ and $\widehat{\boldsymbol{T}}^{\prime}$ are linear reduced order models for the Jacobians of the mechanical and thermal solvers.

Solving Equation~\eqref{}, $\Delta\hat{\mathbf u}$ and $\Delta \hat{\boldsymbol \theta}$  are found to be

$$
\left(\boldsymbol{I}-\widehat{\boldsymbol{T}}^{\prime} \widehat{\boldsymbol{U}}^{\prime}\right) \Delta \hat{\mathbf{u}}_k=-\pazocal{R}_{\theta}\left(\mathbf{u}_k, \boldsymbol{\theta}_k\right)-\widehat{\boldsymbol{T}}^{\prime} \pazocal{R}_{u}\left(\mathbf{u}_k, \boldsymbol{\theta}_k\right)\\
\left(\boldsymbol{I}-\widehat{\boldsymbol{U}}^{\prime} \widehat{\boldsymbol{T}}^{\prime}\right) \Delta \hat{\boldsymbol{\theta}}_k=-\pazocal{R}_{u}\left(\mathbf{u}_k, \boldsymbol{\theta}_k\right)-\widehat{\boldsymbol{U}}^{\prime} \pazocal{R}_{\theta}\left(\mathbf{u}_k, \boldsymbol{\theta}_k\right)
$$

Since neither $\hat{\mathbf U}‚Äô$ nor $\hat{\mathbf T}'$ are known they must be approximated. A Gauss-Seidel like scheme will be employed, in the sense that the equations are solved one after the other with the most up to date values for $\mathbf u$ and $\boldsymbol \theta$ being used for the approximations. Thus assuming that $\mathbf u_k$ and $\boldsymbol \theta_k$ are known, they are used to compute the approximations $\hat{\mathbf U}_k$ and $\hat{\mathbf T}_k$ and $\mathbf u_{k+1}$ is found from

$$
\left(\boldsymbol{I}-\widehat{\boldsymbol{T}}_{k}^{\prime} \widehat{\boldsymbol{U}}_{k}^{\prime}\right) \Delta \hat{\mathbf{u}}_k=-\pazocal{R}_{\theta}\left(\mathbf{u}_{k}, \boldsymbol{\theta}_{k}\right)-\widehat{\boldsymbol{T}}_{k}^{\prime} \pazocal{R}_{u}\left(\mathbf{u}_{k}, \boldsymbol{\theta}_{k}\right)\\ \mathbf{u}_{k+1} = \mathbf{u}_k + \Delta \hat{\mathbf{u}}_k
$$

Now, $\mathbf u_{k+1}$ can be used to improve the approximation of $\hat{\mathbf U}$, denoted $\hat{\mathbf U}_{k+1}$. Thus, $\boldsymbol \theta_{k+1}$ can now be found from

$$
\left(\boldsymbol{I}-\widehat{\boldsymbol{U}}_{k+1}^{\prime} \widehat{\boldsymbol{T}}_{k}^{\prime}\right) \Delta \hat{\boldsymbol{\theta}}=-\pazocal{R}_{u}\left(\mathbf{u}_{k}, \boldsymbol{\theta}_{k}\right)-\widehat{\boldsymbol{U}}_{k+1}^{\prime} \pazocal{R}_{\theta}\left(\mathbf{u}_{k}, \boldsymbol{\theta}_{k}\right)
$$

and the approximation to the thermal Jacobian updated to $\hat{\mathbf T}_{k+1}$.

It finally remains to precise how are $\hat{\mathbf U}$ and $\hat{\mathbf T}$ approximated. To achieve this consider the intermediate values for $\mathbf u$ and $\boldsymbol \theta$ computed as

$$
\begin{array}{l}
\tilde{\mathbf u}_i = \pazocal U(\boldsymbol \theta_i)\\[5pt]
\tilde{\boldsymbol \theta}_i = \pazocal T(\mathbf u_i)
\end{array}\quad \text{for }i=1,\dots,k,
$$

Firstly, to approximate $\hat{\mathbf U}$, consider the matrices defined as

$$
\mathbf V_U \equiv \{\Delta \boldsymbol \theta_1, \Delta \boldsymbol \theta_2, \dots, \Delta \boldsymbol \theta_k\},\\
\mathbf W_U \equiv \{\Delta \tilde{\mathbf u}_1, \Delta \tilde{\mathbf u}_2, \dots, \Delta\tilde{\mathbf u}_k\}
$$

where the deltas $\Delta \boldsymbol \theta_i = \boldsymbol \theta_{i+1} - \boldsymbol \theta_i$ and $\Delta \tilde{\mathbf u}_i = \tilde{\mathbf u}_{i+1} -\tilde{\mathbf u}_i$ are collected into columns.

A new temperature input delta $\Delta \boldsymbol \theta$ can be approximated by a linear combination of the columns of $\mathbf V_U$ as

$$
\mathbf V_U\mathbf c_U\approx \Delta \boldsymbol\theta
$$

and a corresponding output delta $\Delta \mathbf u$ can then be constructed by using the computed coefficients $\mathbf c_U$to linearly combine the columns of $\mathbf W_U$ as

$$
\Delta\mathbf u = \mathbf W_U\mathbf c_U
$$

The $\mathbf c_F$ are determined from a minimization

$$
\mathbf c_U = \arg\min_{\tilde{\mathbf c} } \|\Delta\boldsymbol \theta - \mathbf V_U\tilde{\mathbf c}\|
$$

which is a least-squares problem, solved in \cite{vierendeels} by the normal equations and in \cite{} using the economy-size $QR$-decomposition. The action of $\hat{\mathbf U}'$ on a vector can then be written as

$$
\hat{\mathbf U}'\Delta\boldsymbol \theta = \mathbf W_U \mathbf c_U
$$

Following \cite{vierrendeels}, the solution for the least-squares problem is

$$
\mathbf c_U = (\mathbf V_U^T\mathbf V_U)^{-1}\mathbf V_U\Delta \boldsymbol \theta
$$

Thus, $\hat{\mathbf U}'$ is found to be

$$
\hat{\mathbf U}' = \mathbf W_U(\mathbf V_U^T \mathbf V_U)^{-1}\mathbf V_U
$$

The process to set up the reduced order model for the Jacobian of the thermal solver $\hat{\mathbf T}$ is analogous to that described for the mechanical solver. Input deltas $\Delta \mathbf u_k \equiv \mathbf u_{k+1} - \mathbf u_k$ and output deltas $\Delta\tilde{\boldsymbol \theta} \equiv \tilde{\boldsymbol \theta}_{k+1} - \tilde{\boldsymbol \theta}_k$ are collected in matrices $\mathbf V_T$ and $\mathbf W_T$. After solving a corresponding least-squares problem to find the optimal coefficients $\mathbf c_T$, the action of $\hat{\mathbf T}'$ on a vector is written as

$$
\hat{\mathbf S}\Delta \mathbf u = \mathbf W_T\mathbf c_T
$$

Box~\ref{} shows this method for one timestep. Initially, a prediction $\boldsymbol \theta_p$ is computed by extrapolation from old timesteps and two block Gauss-Seidel iterations are performed using constant underrelaxation. Then, the initial reduced order models for mechanical and thermal systems are set up and the main coupling iteration loop with computation steps as described above starts.

 Quasi-Newton methods

- Vierendeels et al. (2007)
- Degroote et al. (2008)
- Haelterman et al. (2009)
- Erbst and D√ºster (2012)
- Gatzhammer (2014)
- Erbts et al. (2015)
- Wendt et al. (2015)

 Newton-Krylov methods

- Michler et al. (2005)
- K√ºttler and Wall (2009)
- Gatzhammer (2014)
- K√∂nig et al. (2016)
- Scheufele (2018)

 GMRES

Applying the GMRES to the thermo-mechanical problem, the Krylov-subspace of order $m$ is written as

$$
\pazocal{K}_{m}:=\operatorname{span}\left\{\boldsymbol{s}_{i}-\boldsymbol{s}_{k}\right\}_{i=1}^{m}=\operatorname{span}\left\{\Delta \boldsymbol{s}_{i}\right\}_{i=1}^{m},
$$

---

1. $n = 0$
2. Solve for

 Andersen Acceleration

- Uekermann (2016)

 Generalized Broyden

- Uekermann (2016)

 Filtering

 Multi-Scale


\section{Multipoint iteration functions}

\subsection{Extrapolation tecniques with cycling}

According to \cite{brezinski}, there is a very strong connection between sequence transformations and fixed point methods for solving \(x=g(x)\) where \(g: R^{p} \to R^{p}\).
The most well known example of this connection is that between Aitken's \(\Delta^{2}\) process and Steffensen's method in the case \(p=1\).
We have
\[T_{n}=S_{n}-\frac{\left(S_{n+1}-S_{n}\right)^{2}}{S_{n+2}-2 S_{n+1}+S_{n}}, \quad n=0,1, \ldots \quad\text{for Aitken's process}\]
and
\[x_{n+1}=x_{n}-\frac{\left(g\left(x_{n}\right)-x_{n}\right)^{2}}{g\left(g\left(x_{n}\right)\right)-2 g\left(x_{n}\right)+x_{n}}, n=0,1, \ldots \quad\text{for Steffensen's method.}\]
More generally let \(T:\left(S_{n}\right) \to\left(T_{n}\right)\) be a quasi-linear sequence transformation defined by
\[
T_{n}=F\left(S_{n}, \ldots, S_{n+k}\right), \quad n=0,1, \ldots
\]
For solving the fixed point problem \(x=g(x)\) we can associate to it the iterative method
\[
x_{n+1}=F\left(x_{n}, g\left(x_{n}\right), \ldots, g_{k}\left(x_{n}\right)\right), \quad n=0,1, \ldots
\]
where \(g_{i+1}(t)=g\left(g_{i}(t)\right)\) and \(g_{0}(t)=t\).
Conversely to any fixed point iteration of this form, we can associate a sequence transformation of the previous form.

Variations on this approach can be found which include \cite{irons_version_1969} and \cite{king}

There is no universal accelerator \cite{brezinski}

%There is a connection between acceleration of series and fixed point methods

% Describe cycling
% Differnce to common

% Scalar acceleration

% Vector acceleration

More generally let \(T:\left(S_{n}\right) \longmapsto\left(T_{n}\right)\) be a quasi-linear sequence transformation defined by
\[
T_{n}=F\left(S_{n}, \ldots, S_{n+k}\right), \quad n=0,1, \ldots
\]
For solving the fixed point problem \(x=g(x)\) we can associate to it the iterative method
\[
x_{n+1}=F\left(x_{n}, g\left(x_{n}\right), \ldots, g_{k}\left(x_{n}\right)\right), \quad n=0,1, \ldots
\]
where \(g_{i+1}(t)=g\left(g_{i}(t)\right)\) and \(g_{0}(t)=t\). Conversely to any fixed point iteration of this form, we can associate a sequence transformation of the previous form.
\jvc{Include Aitken-Steffson connection?}

\subsubsection{Constant Underrelaxation}

One of the most straightforward ways to stabilize an iterative method is to use constant underrelaxation. \cite{gatzhammer_efficient_2014}
The relaxation is performed as follows
\begin{equation} \label{eq:constant_relaxation}
\mathbf x^{k+1}=(1-\omega) \mathbf x^{k}+\omega(\mathbf x^k - \pazocal R(\mathbf x^k))=\mathbf x^{k} -\omega \mathbf{r}^{k+1},
\end{equation}
where \(\omega\) is the relaxation factor chosen in the range \(0<\omega<1\), which corresponds to an underrelaxation, to achieve a stabilizing effect.

Applying to Equation~\eqref{eq:def_res_jacobi}
\begin{equation}
  \left\{\begin{array}{c}
    \mathbf u^{k+1}\\
    \bm \uptheta^{k+1}
  \end{array}\right\} =
  (1-\omega)
  \left\{\begin{array}{c}
    \mathbf u^{k}\\
    \bm \uptheta^{k}
  \end{array}\right\}
  + \omega
  \left\{\begin{array}{c}
    \pazocal U(\bm\uptheta^k)\\
    \pazocal T(\mathbf u^k)
  \end{array}\right\}
\end{equation}
Applying to Equation~\eqref{eq:def_res_gauss_seidel}
\begin{equation}
  \bm\uptheta^{k+1} = (1-\omega)\bm\uptheta^k + \omega \pazocal T\left(\pazocal U(\bm \uptheta^k)\right).
\end{equation}


Constant underrelaxation works well if \(\omega\) is close to 1 , but leads to a slow convergence if \(\omega\) has to be chosen close to 0.
Thus, the constant underrelaxation method leads to unmanageable computational costs for severe instabilities.
The optimal \(\omega\) is not necessary the largest stable one \citet{gatzhammer_efficient_2014} and has to be set empirically.
In what follows, alternative methods are discussed, which try to decrease the number of iterations necessary while still keeping stability.

\begin{framedbox}[htb]
  \caption{Constant underrelaxation applied to the block Gauss-Seidel scheme.}
  \label{box:constant_underrelaxation}
  \begin{center}
    \begin{minipage}{0.9\textwidth}
    \begin{enumerate}[(i)]
    \item \(\bm\uptheta^0 = \bm\uptheta_{n+1}^p\)
    \item Set fixed-point counter to zero: \(k=0\)
    \item Enter the fixed-point loop
    \begin{enumerate}[(1)]
      \item Solve the mechanical problem at fixed temeperature \(\bm \uptheta^k\): \(\mathbf u^{k+1} = \pazocal U(\bm \uptheta^k)\)
      \item Solve the thermal problem at a fixed configuration \(\mathbf u^{k+1}\): \(\bm \uptheta^{k+1} = \pazocal T(\mathbf u^{k+1})\)
      \item Compute \(\bm \uptheta^{k+1}\) using constant relaxation (Equation~\eqref{eq:constant_relaxation})
      \item If the desired accuracy has not been reached, update \(k=k+1\) and go to step (1).
    \end{enumerate}
    \end{enumerate}
    \end{minipage}
  \end{center}
\end{framedbox}

\subsubsection{Aitken \(\delta^2\) method}

The formula for the one-dimensional case is

$$
AX_n = \frac{x_n x_{n+2} - x_{n+1}^2}{x_n-2x_{n+1}+ x_{n+2} }
$$

or

$$
AX_n = x_{n+2} - \frac{x_{n+2} - x_{n+1} }{x_n + x_{n+2} - 2 x_{n+1} }(x_{n+1}-x_{n+2})
$$

Aitken's method will accelerate the sequence $x_{n}$ if $\lim_{n \rightarrow \infty} \frac{(A x)_{n}-\ell}{x_{n}-\ell}=0$.
$A$ is not a linear operator, but a constant term drops out, viz: $A[x-\ell]=A x-\ell$, if $\ell$ is a constant. This is clear from the expression of $A x$ in terms of the finite difference operator $\Delta$.
Although the new process does not in general converge quadratically, it can be shown that for a fixed point process, that is, for an iterated function sequence $x_{n+1}=f\left(x_{n}\right)$ for some function $f$, converging to a fixed point, the convergence is quadratic. In this case, the technique is known as Steffensen's method.

Empirically, the $A$-operation eliminates the "most important error term". One can check this by considering a sequence of the form $x_{n}=\ell+a^{n}+b^{n}$, where $0<b<a<1$: The sequence $A x$ will then go to the limit like $b^{n}$ goes to zero.

Geometrically, the graph of an exponential function $f(t)$ that satisfies \(f(n)=x_{n}, f(n+1)=x_{n+1}\) and \(f(n+2)=x_{n+2}\) has an horizontal asymptote at \(\frac{x_{n} x_{n+2}-x_{n+1}^{2}}{x_{n}-2 x_{n+1}+x_{n+2}}\left(\right.\) if \(\left.x_{n}-2 x_{n+1}+x_{n+2} \neq 0\right)\).

If you want to fit \(a e^{b x}+c\) to 3 points, \(\left(x_{i}, y_{i}\right)*{i=1}^{3}\), you want \(y*{i}=a e^{b x_{i}}+c .\)
Subtracting the first two, \(y_{2}-y_{1}=a\left(e^{b x_{2}}-e^{b x_{1}}\right)=a e^{b x_{1}}\left(e^{b\left(x_{2}-x_{1}\right)}-1\right)\) and, similarly, \(y_{3}-y_{2}=a\left(e^{b x_{3}}-e^{b x_{2}}\right)=a e^{b x_{2}}\left(e^{b\left(x_{3}-x_{2}\right)}-1\right) .\)
If \(x_{3}-x_{2}=x_{2}-x_{1}=d\), so the data are equally spaced and \(e^{b\left(x_{2}-x_{1}\right)}-1=e^{b\left(x_{3}-x_{2}\right)}-1\), we can divide these to get
\(\frac{y_{3}-y_{2}}{y_{2}-y_{1}}=e^{b x_{2}-b x_{1}}=e^{b\left(x_{2}-x_{1}\right)}=e^{b d}\)
Taking logs, and letting \(r=\frac{y_{3}-y_{2}}{y_{2}-y_{1}}, \ln (r)=b d\) so \(b=\frac{\ln (r)}{d} .\) From this, \(e^{b}=r^{1 / d} .\)
Note that this requires that \(r>0\), so the points are monotonic.
Then
\[
\begin{aligned}
y_{2}-y_{1} &=a\left(e^{b x_{2}}-e^{b x_{1}}\right) \\
&=a\left(r^{x_{2} / d}-r^{x_{1} / d}\right)
\end{aligned}
\]
so that
\[
a=\frac{y_{2}-y_{1}}{r^{x_{2} / d}-r^{x_{1} / d}}
\]
We also get
\[
a=\frac{y_{3}-y_{2}}{r^{x_{3} / d}-r^{x_{2} / d}}=\frac{y_{3}-y_{1}}{r^{x_{3} / d}-r^{x_{1} / d}}
\]
Finally, \(c=y_{i}-a e^{b x_{i}}\) for any \(i\).

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/def2fba4-2bd4-4a98-8216-4d44881db9ff/Untitled.png)

One can also show that if \(x\) goes to its limit \(\ell\) at a rate strictly greater than \(1\)\footnote{$\left\{x_{n}\right\}{n \in \mathbb{N}}$ converges linearly to $\ell$ if there exists a number $\mu \in(0,1)$ such that \(\lim_{n \rightarrow \infty} \frac{\left|x_{n+1}-\ell\right|}{\left|x_{n}-\ell\right|}=\mu\).}, \(A x\) does not have a better rate of convergence.
(In practice, one rarely has e.g. quadratic convergence which would mean over 30 resp. 100 correct decimal places after 5 resp. 7 iterations (starting with 1 correct digit); usually no acceleration is needed in that case.)

In practice, \(A x\) converges much faster to the limit than \(x\) does, as demonstrated by the example calculations below. Usually, it is much cheaper to calculate \(A x\) (involving only calculation of differences, one multiplication and one division) than to calculate many more terms of the sequence \(x\). Care must be taken, however, to avoid introducing errors due to insufficient precision when calculating the differences in the numerator and denominator of the expression.


\subsubsection{Vector Extrapolation}

$$
\hat{\boldsymbol{s}}_{k+m}=\boldsymbol{s}_{k+1}+\sum_{i=1}^{m-1} \omega_{i}\left(\boldsymbol{s}_{k+i+1}-\boldsymbol{s}_{k+i}\right)=\boldsymbol{s}_{k+1}+\sum_{i=1}^{m-1} \omega_{i} \Delta \boldsymbol{s}_{k+i+1}
$$

$$
\begin{aligned}\Delta \hat{\boldsymbol{s}}_{k+m} &=\sum_{i=0}^{m-1} \gamma_{i} \Delta \boldsymbol{s}_{k+i+1} \\& \sum_{i=0}^{m-1} \gamma_{i}=1\end{aligned}
$$

\[
A \gamma=0
\]
factors \(\gamma_{i}\). The difference between th f the system matrix \(\boldsymbol{A}\). This is done
\[
a_{i j}=\Delta s_{k+i} \cdot \Delta s_{k+j}
\]
extrapolation (MPE) method, as
\[
a_{i j}=\left(\Delta s_{k+i}-\Delta s_{k+i-1}\right) \cdot \Delta s_{k+j}
\]
lation (RRE) method, and as
\[
a_{i j}=\boldsymbol{y}^{i} \cdot \Delta \boldsymbol{s}_{k+j}
\]

\subsection{GMRES-inspired approach}

In \cite{michler_interface_2005}, a Krylov-subspace method is proposed in the context of FSI.
In the present document, this mehod is adapted to the thermo-mechanical problem.
It approximates the solution $\Delta \bm{\uptheta}^{k}$ of system \eqref{eq:newton_raphson}, which results in a Newton-Krylov solver for the partitioned thermo-mechanical system.
This particular nomenclature is however debatable.
\cite{kuttler_vector_2009} argues that the correct term for this approach should be instead a ‚ÄúKrylov-based vector extrapolation‚Äù method. \jvc{Later take a closer look a this and link to the section on vector extrapolation.}

% A review of Newton-Krylov methods in general is given in [93].

For the GMRES-inspired approach, the Krylov-based subspace of order $m$ is written as
\begin{equation}
\pazocal{K}_{m}:=\operatorname{span}\left\{\boldsymbol{\theta}^{*}_i-\boldsymbol{\theta}^{k}\right\}_{i=1}^{m}=\operatorname{span}\left\{\Delta \boldsymbol{\theta}^*_{i}\right\}_{i=1}^{m},
\end{equation}
where $\Delta \bm{\uptheta}^*_{i}:=\bm{\uptheta}^*_{i}-\bm{\uptheta}^{k}$ and the $\bm{\uptheta}^*_{i}$ are generated during the Krylov iterations as $\bm{\uptheta}^*_{i+1}=\pazocal{T} \circ \pazocal U\left(\bm{\uptheta}^*_{i}\right)$ . $\bm{\uptheta}^{k}$ is associated to the outer Newton iteration and fixed during the Krylov iterations.

The residual for the $m^\mathrm{th}$ Krylov iteration can be written as
\begin{equation}
J_\pazocal{R}(\boldsymbol \theta^k)\Delta\boldsymbol \theta^*_m - \pazocal R(\boldsymbol \theta^k).
\end{equation}
It is desirable that the $L_2$-norm of this residual be as small as possible.
Approximating $\Delta \bm \uptheta^*_i$ using $\pazocal K_m$, as $\sum_{i=1}^m \alpha_i \Delta \bm \uptheta^*_i$, the coefficients $\alpha_i$ are thus found as
\begin{equation} \label{eq:gmres_1st_ls}
\bar{\boldsymbol \alpha} = \arg\min_{\boldsymbol\alpha} \left\|J_\pazocal{R}(\bm \uptheta^k)\sum_{i=1}^m \alpha_i \Delta\bm \uptheta^i - \pazocal R(\bm \uptheta^k)\right\|.
\end{equation}

Since the Jacobian matrix $J_{\pazocal R}$ is assumed not to be accessible, a finite-difference approach
\begin{equation}
J_{\pazocal{R}}\left(\boldsymbol{\theta}^{k}\right) \Delta \boldsymbol{\theta}^*_{i} \approx \pazocal{R}\left(\boldsymbol{\theta}^*_{i}\right)-\pazocal{R}\left(\boldsymbol{\theta}^{k}\right)
\end{equation}
is employed.
Using the notation
\begin{equation}
\mathbf r^*_i = \pazocal R(\boldsymbol \theta^*_i),\quad
\mathbf r^k \equiv  \pazocal R(\boldsymbol \theta^k),\quad
\Delta \mathbf r_i^* \equiv \mathbf r_i^* - \mathbf r^k,
\end{equation}
Equation~\eqref{eq:gmres_1st_ls} can be rewritten in a more compact form as
\begin{equation} \label{eq:gmres_ls_condition}
\bar{\boldsymbol \alpha} = \arg\min_{\boldsymbol \alpha} \left\|\mathbf r^k+\sum_{i=1}^m\alpha_i\Delta\mathbf r^*_i\right\|,
\end{equation}
with the $\alpha_i$ determined in a least-squares sense.
The quality of the approximation is determined from the norm of the residual
\begin{equation} \label{eq:gmres_residual}
\xi=\left\|\boldsymbol{r}^{k}+\sum_{i=1}^{m} \bar{\alpha}_{i} \Delta \boldsymbol{r}^*_{i}\right\|.
\end{equation}

Orthogonalizing a new Krylov-vector $\Delta \bm{\uptheta}^*_{m}$ with respect to all previous ones using the Arnoldi process, as shown in Box~\ref{box:arnoldi_process}, completes the connection to the generalized minimal residual method (GMRES).
Furthermore, to stabilize the solver subiterations when setting up the Krylov-subspace approximation, constant underrelaxation by $\omega$ can be employed as
\begin{equation}
\Delta \boldsymbol{\theta}^*_{m}=\omega \Delta \boldsymbol{\theta}^*_{m}.
\end{equation}

\begin{framedbox}[htbp]
  \caption{Arnoldi process to orthonormalize temperature deltas}
  \label{box:arnoldi_process}
  \begin{center}
    \begin{minipage}{0.9\textwidth}
    \begin{enumerate}[(i)]
      \item \(j=1\)
      \item \(\Delta \bm\uptheta^*_m = \Delta \bm\uptheta^*_m - \Delta \bm\uptheta^*_j\frac{\Delta\bm\uptheta^*_m\cdot\Delta\bm\uptheta^*_j}{\Delta\bm \uptheta^*_j\cdot\Delta\bm\uptheta^*_j}\)
      \item \(j=j+1\)
      \item If \(j<m-1\) go to Step (ii)
      \item \(\Delta\bm \uptheta^*_m = \Delta\bm\uptheta^*_m/\|\Delta \bm \uptheta^*_m\|\)
    \end{enumerate}
    \end{minipage}
  \end{center}
\end{framedbox}


Box~\ref{box:gmres_inspired} puts all parts of the GMRES-inpired method for one coupling timestep in proper order. The Krylov-vectors can be reused between different coupling iterations $k$ or even between different timesteps $n$.

\begin{framedbox}[htbp]
  \caption{Timestep \(n\) of the GMRES-inspired approach.}
  \label{box:gmres_inspired}
  \begin{center}
    \begin{minipage}{0.9\textwidth}
    \begin{enumerate}[(i)]
    \item \(k=0\)
    \item If no previous timesteps are to be reused or \(n=0\) then
    \begin{itemize}
      \item Clear all \(\Delta \bm \uptheta^*_i\), \(\Delta \mathbf r^*_i\)
      \item \(m=0\)
    \end{itemize}
    \item \(\bm \uptheta^*_1 = \pazocal T\circ \pazocal U(\bm \uptheta^0)\)
    \item \(\mathbf r^0 = \bm \uptheta^*_1 - \bm \uptheta^0\)
    \item Enter the Newton loop
    \begin{enumerate}[(1)]
      \item If no previous iterations are to be reused
      \begin{itemize}
        \item Clear all \(\Delta \bm\uptheta^*_i\), \(\Delta \mathbf r^*_i\)
        \item \(m=0\)
        \item \(\xi = \|\mathbf r^*_k\|\)
      \end{itemize}
      else
      \begin{itemize}
        \item Compute \(\bar{\mathbf\alpha}\) (Equation~\eqref{eq:gmres_ls_condition}) and \(\xi\) (Equation~\eqref{eq:gmres_residual})
        \item \(\bm \uptheta^*_{m+1} = \bm \uptheta^*_1\)
      \end{itemize}
      \item Enter the Krylov loop
      \begin{enumerate}[(a)]
        \item \(m=m+1\)
        \item \(\Delta \bm\uptheta^*_m = \bm\uptheta^*_m - \bm\uptheta^k\)
        \item Orthogonalize (Box~\ref{box:arnoldi_process}) and relax \(\Delta \bm\uptheta^*_m\) (Equation~\eqref{})
        \item \(\bm\uptheta^*_m = \bm\uptheta^k + \Delta\bm\uptheta^*_m\)
        \item \(\bm \uptheta^*_{m+1} = \pazocal T \circ \pazocal U(\bm \uptheta^*_m)\)
        \item \(\Delta \mathbf r^*_m = (\bm\uptheta^*_{m+1} - \bm \uptheta^*_m) - \mathbf r^k\)
        \item Compute \(\bar{\mathbf\alpha}\) (Equation~\eqref{eq:gmres_ls_condition}) and \(\xi\) (Equation~\eqref{eq:gmres_residual})
        \item If convergence has not been reached, \(\xi>\epsilon_2\), go to Step (a)
      \end{enumerate}
    \item \(\bm\uptheta^{k+1} = \bm\uptheta^k + \sum_{i=1}^m \bar{\alpha}_i \Delta\bm\uptheta_i\)
    \item \(k=k+1\)
    \item \(\bm\uptheta^*_1 = \pazocal T\circ \pazocal U(\bm\uptheta^{k})\)
    \item \(\mathbf r^k = \bm\uptheta^*_1 - \bm \uptheta^k\)
    \item If convergence has not been reached, \(\|\mathbf r^k\| > \epsilon_1\), go to Step (1)
    \end{enumerate}
    \end{enumerate}
    \end{minipage}
  \end{center}
\end{framedbox}

% \Floatbarrier

\section{Multipoint iteration functions with memory}
